# Hugging Face Daily Papers Report
**Date**: 2026-02-23
**Source URL**: https://huggingface.co/papers/date/2026-02-23

============================================================

## 📄 SARAH: Spatially Aware Real-time Agentic Humans

- **链接**: https://huggingface.co/papers/2602.18432
- **阅读来源**: HTML

# SARAH: Spatially Aware Real-time Agentic Humans 研究报告

### 1. 应用领域
**计算机视觉 / 计算机图形学** - 3D数字人动作生成、虚拟现实（VR）交互、具身智能（Embodied AI）。

### 2. 一句话核心贡献
提出了首个可实时运行的完全因果（Fully Causal）3D全身动作生成方法，解决了虚拟代理在对话中无法根据用户移动进行空间定位、身体朝向调整和视线交互的问题。

### 3. 使用指南
*   **输入数据**：
    1.  **用户位置**：用户在地面的投影头部轨迹（User's floor-projected head trajectory）。
    2.  **双向音频**：对话双方（用户和代理）的音频信号。
    3.  **控制参数（可选）**：推理时的注视评分（Gaze Score），用于调节眼神接触的强度。
*   **输出结果**：虚拟代理（Agent）的全身3D动作序列，包括身体朝向和姿态。
*   **硬件与性能**：该方法支持在VR头显上进行流式推理，推理速度超过 **300 FPS**（在A100 GPU上测试），实现了真正的实时交互。
*   **部署方式**：系统基于流式推理设计，无需访问未来帧信息，可集成到实时Avatar系统中。

### 4. 主要创新点
1.  **实时因果架构（Causal Architecture）**：
    提出了一种结合了**因果Transformer VAE**与**流匹配（Flow Matching）**模型的架构。通过在时间上交错排列潜在Token（Interleaved Latent Tokens），实现了在不牺牲时间连贯性的前提下的流式推理，避免了非因果模型需要预知未来的限制。
2.  **学习与控制解耦的注视引导机制**：
    引入了基于**无分类器引导（Classifier-Free Guidance, CFG）**的注视评分机制。模型从数据中学习自然的分布，但在推理时允许用户通过调节参数来控制代理与用户眼神接触的强度（从回避视线到持续对视），增强了交互的适应性。
3.  **全欧几里得运动表征（Fully Euclidean Representation）**：
    放弃了传统的关节角度表示，转而使用**欧几里得表面点（Surface-point）**表示法（将每个关节编码为3D二十面体顶点）。这种方法提高了训练的稳定性，确立了更精确的末端执行器控制，并减少了传统正向运动学带来的误差累积。

### 5. 实验效果
在 **Embody 3D** 数据集（包含50小时的多视角双人对话数据）上进行了评估，主要表现如下：
*   **速度性能**：推理速度超过 **300 FPS**，比非因果基线（如MDM, Audio2Photoreal）快 **3倍**以上。
*   **动作质量**：在生成动作的真实感（FGD指标）和空间感知能力上达到了 State-of-the-Art (SOTA) 水平。
*   **空间对齐**：在不访问未来用户位置的情况下，其视线对齐（Gaze Alignment）精度能够匹敌非因果方法（即那些可以看到未来的方法），证明了模型成功学习到了反应性的空间行为。
*   **用户验证**：已在实时VR系统中成功部署，生成的代理能够自然地转向用户并根据用户移动调整姿态，同时保持自然的对话手势。


============================================================

## 📄 Selective Training for Large Vision Language Models via Visual Information Gain

- **链接**: https://huggingface.co/papers/2602.17186
- **阅读来源**: HTML

### 1. 应用领域
**多模态大语言模型 (LVLM) 的指令微调与数据筛选**。具体涉及通过量化视觉依赖性来减少模型幻觉（Hallucination）和缓解语言偏见（Language Bias）。

### 2. 一句话核心贡献
提出了一种名为“视觉信息增益 (VIG)”的度量指标，通过量化样本和Token对视觉输入的依赖程度，实现了在仅使用极少量高视觉信息含量的Token（如总量的18%）进行训练的情况下，显著提升模型性能并减少幻觉。

### 3. 使用指南
*   **输入**：
    *   预训练好的多模态模型（即已完成视觉-语言对齐阶段，如Pre-trained LLaVA-1.5）。
    *   多模态指令微调数据集（图像-文本对）。
*   **流程**：
    1.  **计算 VIG**：对于每个训练样本，分别计算模型在给定原始图像和模糊图像（模拟无视觉输入）条件下生成答案的困惑度（Perplexity）。VIG 定义为两者困惑度的对数比，数值越高代表视觉信息对该预测越关键。
    2.  **样本筛选**：根据样本级 VIG 分数，保留视觉依赖性较高的样本（例如前70%）。
    3.  **Token 级掩码**：在保留的样本中，进一步计算每个 Token 的 VIG，训练时仅计算高 VIG Token（如颜色、位置、属性等）的 Loss，屏蔽介词、助词等纯文本依赖的 Token。
    4.  **训练**：使用上述筛选后的稀疏监督信号进行指令微调。
*   **输出**：视觉锚定能力更强、幻觉更少的 LVLM。
*   **硬件要求**：需要 GPU 进行前向推理以计算数据集的 VIG 分数，以及后续的模型微调（文中 VIG 计算使用 RTX 4090，微调使用 A100）。

### 4. 主要创新点
1.  **视觉信息增益 (VIG) 指标**：提出了一种基于模型不确定性减少量的度量方法。不同于以往仅依赖注意力机制的定性分析，VIG 提供了一种定量的、可分解到 Token 级别的指标，能精确区分视觉强相关内容（如物体颜色）和文本先验内容（如语法结构）。
2.  **双层级选择性训练策略**：设计了“样本级过滤 + Token 级掩码”的组合策略。这不仅剔除了文本主导的低质量样本，更在训练目标中剔除了非视觉依赖的 Token，迫使模型在优化过程中专注于视觉证据，而非语言捷径。
3.  **极高的数据效率与正交性**：证明了仅利用数据集中极小部分的“有效 Token”（例如 LLaVA-1.5 7B 中仅约 17.6% 的 Token），即可超越全量数据训练的效果。同时，该方法作为数据层面的改进，可与 VCD、LACING 等模型/推理层面的去偏方法正交组合，进一步提升性能。

### 5. 实验效果
在 LLaVA-1.5 (7B, 13B) 和 ShareGPT4V 等模型上进行了广泛验证，主要结果如下：
*   **性能提升**：在仅使用 **~5% 至 ~18%** 的 Token 参与梯度更新的情况下，模型在通用视觉理解基准（LLaVA-Bench, MM-Vet, MMBench, DocVQA）上的表现全面超越全量数据训练的基线模型。
*   **幻觉抑制**：在 POPE、CHAIR 和 MMHal 等幻觉评估基准上，VIG 训练的模型显著优于基线，表明模型减少了对文本先验的盲目依赖。
*   **注意力机制改善**：层级注意力分析显示，VIG 训练后的模型在中间层（负责语义特征提取的关键层）对视觉 Token 分配了显著更高的注意力权重。


============================================================

## 📄 Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty

- **链接**: https://huggingface.co/papers/2602.18312
- **阅读来源**: HTML

1. **应用领域**：
强化学习（Reinforcement Learning）- 机器人控制与基于物理的角色动画（Physics-based Character Animation & Robotics Control）。

2. **一句话核心贡献**：
提出了一种线性策略网络（Linear Policy Net, LPN）结合动作雅可比惩罚（Action Jacobian Penalty）的方法，在极低计算开销下有效消除了强化学习策略中的不自然高频抖动信号，实现了无需针对特定任务调参的高质量平滑运动控制。

3. **使用指南**：
*   **输入**：模拟角色的状态（如根节点位置/姿态、关节角度/速度的最小坐标表示）以及参考动作的目标状态。
*   **模型架构**：构建一个线性策略网络（LPN），其主干为一个全连接神经网络（MLP）。该 MLP 不直接输出动作，而是输出一个反馈矩阵 $\mathbf{K}_t$ 和前馈项 $\mathbf{k}_t$。
*   **输出与计算**：最终控制动作通过线性反馈公式计算：$\mathbf{a}_t = \mathbf{K}_t \mathbf{s}_t + \mathbf{k}_t + \hat{\mathbf{a}}_t$（其中 $\hat{\mathbf{a}}_t$ 为参考动作）。
*   **训练配置**：在标准 PPO 损失函数中增加一项动作雅可比惩罚项（$\mathcal{L}_{\text{Jac}} = \lVert \mathbf{J} \rVert^2$），利用 PyTorch 等框架的自动微分功能直接计算。
*   **硬件需求**：标准 GPU（如 NVIDIA RTX A6000）用于网络推理与优化，CPU 用于物理仿真环境采样。

4. **主要创新点**：
*   **动作雅可比惩罚（Action Jacobian Penalty）**：提出直接惩罚动作相对于状态的雅可比矩阵范数，从根本上抑制了策略对输入微小变化的过度敏感，相比传统的“动作变化率惩罚”更能有效消除高频噪声且无需繁琐调参。
*   **线性策略网络（Linear Policy Net, LPN）**：设计了一种特殊的网络架构，通过输出时变线性反馈控制器参数而非直接动作，将计算动作雅可比及其梯度的复杂度从昂贵的反向传播降低为简单的前向/后向传递，解决了在标准全连接网络上应用雅可比惩罚计算成本过高的问题。
*   **高效性与表达能力的平衡**：研究发现受限的线性反馈形式并未降低策略的学习能力，反而能以更快的收敛速度学会包括后空翻、跑酷（Vault/Climb）等高动态技能，并能直接部署到物理四足机器人上。

5. **实验效果**：
*   **仿真环境表现**：在 DeepMimic 风格的多种运动模仿任务（行走、跑步、后空翻、乒乓球步法、跑酷等）中，LPN 结合雅可比惩罚生成的动作在平滑度指标（如 >10Hz 的能量占比、加加速度 Jerk）上显著优于基线方法。
*   **训练效率**：相比于在标准全连接网络上施加雅可比惩罚（导致训练慢约 3 倍），LPN 的计算开销几乎可以忽略不计，且在收敛速度上快于 Lipschitz 约束策略和标准 PPO 基线。
*   **真机部署**：成功将学习到的策略部署在带有机械臂的波士顿动力 Spot 机器人上，实现了稳定的踏步与挥臂组合动作，验证了其 Sim-to-Real 的鲁棒性。


============================================================

## 📄 EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots

- **链接**: https://huggingface.co/papers/2602.18071
- **阅读来源**: HTML

1. **应用领域**：
机器人学 - 移动操作 (Mobile Manipulation) / 强化学习 (Reinforcement Learning) / 第一人称视觉导航 (Egocentric Vision)

2. **一句话核心贡献**：
提出了一种名为 EgoPush 的策略学习框架，通过限制特权教师的观测范围以匹配学生视野，并利用对象中心化的潜在表示，使移动机器人仅凭第一人称视觉即可在无全局地图和定位的动态环境中完成长视距的多物体推移重排任务。

3. **使用指南**：
*   **输入**：机器人的第一人称 RGB-D 图像（RGB 用于语义分割以获取实例掩码，掩码处理后的 Depth 图作为策略网络的直接输入）。
*   **输出**：移动底盘的连续控制指令（线速度和角速度，进而转换为轮速）。
*   **训练流程**：
    1.  **阶段一**：在仿真环境（如 NVIDIA Isaac Lab）中训练一个拥有特权信息的“教师”策略，输入为稀疏关键点。
    2.  **阶段二**：通过在线 DAgger 风格的蒸馏，将教师策略的行为和潜在空间关系知识迁移给仅依赖视觉的“学生”策略。
*   **硬件需求**：带 RGB-D 相机（如 Intel RealSense）的差分驱动移动机器人（如 TurtleBot），以及用于训练和推理的 GPU 计算资源。

4. **主要创新点**：
*   **受限教师强化学习 (Constrained Teacher RL)**：为了解决全知全能的教师策略产生的轨迹在学生有限视野下无法复现的问题，该方法强制教师策略仅能观测到视锥内的信息（Egocentric Visibility-Limited Observations），从而诱导教师产生可被视觉学生学习的主动感知行为。
*   **对象中心化的潜在空间表示 (Object-Centric Latent Representation)**：设计了一个共享权重的点云编码器，将场景抽象为任务相关的角色（当前操作对象、锚点、障碍物），编码它们之间的相对空间关系而非绝对坐标，使策略不依赖全局状态估计。
*   **阶段性时间衰减奖励 (Stage-Aligned Temporal Credit Assignment)**：针对长视距任务中信用分配困难的问题，将重排任务分解为“接近”和“放置”等子阶段，并引入基于阶段步数的动态衰减奖励，有效指导了长时间跨度下的策略优化。

5. **实验效果**：
*   **仿真性能**：在 NVIDIA Isaac Lab 进行的“十字形重排”任务中，EgoPush 取得了接近 100% 的成功率，而传统的端到端 RL 基线（RGB/RGB-D/RNN）和基于地图规划的基线（SIM）成功率均不足 1%，证明了该方法在长视距、接触丰富任务中的鲁棒性。
*   **蒸馏效率**：消融实验显示，使用受限视角的教师训练出的学生模型成功率达到 70% 以上，而使用全知视角的教师训练出的学生模型成功率为 0%，验证了解决“观测差异”对于策略蒸馏的重要性。
*   **真机迁移**：在 TurtleBot3 平台上实现了零样本（Zero-Shot）Sim-to-Real 迁移，在真实世界 5 个箱子的重排任务中达到了 80% 的成功率。


============================================================

## 📄 Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control

- **链接**: https://huggingface.co/papers/2602.18422
- **阅读来源**: HTML

1. **应用领域**：计算机视觉 - 视频生成、扩展现实 (XR)、具身智能 (Embodied AI)、世界模型 (World Simulators)。

2. **一句话核心贡献**：提出了一种以人为中心的交互式视频世界模型，通过引入混合 2D-3D 手部姿态和显式相机位姿作为条件，实现了支持精细手物交互和视角控制的“生成现实”体验。

3. **使用指南**：
    *   **输入**：用户的实时头部姿态（6-DoF Camera Pose）、手部关节级数据（20个指关节角度及手腕位姿，基于 UmeTrack 模型）、以及文本提示或初始图像。
    *   **输出**：动态响应用户动作的第一视角（Egocentric）合成视频流。
    *   **硬件流程**：使用 VR 头显（如 Meta Quest 3）采集用户动作数据，传输至高性能服务器（如 NVIDIA H100），模型生成视频帧后流式传回头显显示。
    *   **系统性能**：在单张 H100 GPU 上可达到 11 FPS 的生成帧率，端到端延迟约 1.4 秒。

4. **主要创新点**：
    *   **混合 2D-3D 手部条件策略**：系统地评估了多种手部注入方式，提出结合“2D ControlNet 风格骨架图”与“3D 关节参数（HPP）Token 添加”的混合策略，有效解决了纯 2D 表示中的深度歧义和自遮挡问题，实现了高保真手部控制。
    *   **手部与相机联合控制机制**：设计了联合控制架构，将显式相机外参（转换为 Plücker 嵌入）与手部特征在潜在空间进行融合，解决了单一模态控制中手眼不协调和交互对象错误的问题。
    *   **交互式自回归蒸馏系统**：将基于双向注意力的视频扩散教师模型（Wan2.2）蒸馏为因果（Causal）自回归学生模型，构建了闭环实时系统，支持零样本（Zero-shot）环境下的长序列交互生成。

5. **实验效果**：
    *   **数据集表现**：在 HOT3D 和 GigaHands 数据集上，混合条件策略在手部重建准确度（MPJPE, MPVPE）和视频质量（FVD, LPIPS）上均显著优于仅使用 2D 骨架或仅使用 Mask 的基线方法；联合控制模型在相机轨迹和手部姿态准确性上取得了最佳平衡。
    *   **用户研究**：在“按按钮”、“开罐子”等任务中，该系统的任务完成率高达 **71.2%**，而仅依赖文本提示的基线模型仅为 **3.0%**；用户感知控制评分（7分制）也从基线的 1.74 提升至 **4.21**。


============================================================

## 📄 Does Your Reasoning Model Implicitly Know When to Stop Thinking?

- **链接**: https://huggingface.co/papers/2602.08354
- **阅读来源**: HTML

# SAGE-RL 研究报告

1. **应用领域**
   自然语言处理（NLP）- 大型推理模型（LRMs）、强化学习（Reinforcement Learning）、思维链（CoT）推理优化。

2. **一句话核心贡献**
   揭示了大型推理模型隐式具备“知道何时停止思考”的能力，并提出了 SAGE-RL 方法，通过在强化学习的 rollout 阶段引入一种新的采样策略，成功在不牺牲准确率的情况下显著减少了推理步骤的冗余，实现了模型推理效率与性能的双重提升。

3. **使用指南**
   *   **输入**：需要进行复杂逻辑推理的任务（如数学问题、代码生成等）以及一个基础推理模型（如 DeepSeek-R1-Distill 系列）。
   *   **核心流程**：
       *   **推理阶段 (SAGE)**：在解码时不仅仅依赖 Pass@1，而是采用一种基于置信度的探索策略（SAGE），在一定宽度的探索空间内寻找简短且高置信度的推理链。
       *   **训练阶段 (SAGE-RL)**：将 SAGE 采样集成到基于验证奖励的强化学习（RLVR，如 GRPO/GSPO）的 rollout 生成过程中。即在训练时，利用 SAGE 发现的高效推理路径作为正样本来更新策略模型。
   *   **输出**：经过 SAGE-RL 微调后的模型，能够在标准 Pass@1 推理模式下，直接输出更简洁且准确的思维链和答案。
   *   **硬件需求**：标准的 GPU 训练环境（论文中使用 8 张 GPU 进行实验）。

4. **主要创新点**
   *   **实证发现模型具备“适时停止”的潜能**：通过引入 RFCS（首次出现正确答案的步骤索引/总步骤数）指标，通过实验证实 LRMs 内部其实知道何时该停止思考，但这种能力被当前的贪婪或随机采样（Pass@1）策略所掩盖，导致了大量的无效“过度思考”。
   *   **SAGE 采样范式**：提出了一种名为 SAGE 的推理感知采样策略。不同于传统的 Token 级别扩展，SAGE 根据模型自身的自信程度进行推理路径的探索，能够主动发现那些被标准采样忽略的、既简短又正确的推理链。
   *   **SAGE-RL 训练框架**：开发了 SAGE-RL，将 SAGE 发现的高效推理模式无缝集成到 RLVR（如 GRPO）训练中。通过在 Rollout 阶段混合使用 SAGE 采样和随机采样，让模型在训练过程中“学会”这种高效推理模式，从而在推理时无需额外搜索即可生成简洁答案。

5. **实验效果**
   *   **数据集**：在 6 个具有挑战性的数学基准测试上进行了评估，包括 MATH-500, AIME 2024, AIME 2025, AMC23, OlympiadBench 和 Minerva。
   *   **基础模型**：涵盖 DeepSeek-R1-Distill-Qwen (1.5B, 7B)、DeepScaleR 和 Qwen3-8B。
   *   **性能提升**：
       *   **准确率**：SAGE-RL 在所有测试基准上均取得了一致的性能提升。例如，使用 SAGE-GSPO 在 AIME 2025 上 Pass@1 提升了 6.2%。
       *   **效率提升**：显著缩短了响应长度（Response Length），提高了 Token 效率（Pass@1 / Length）。在大多数模型和基准上，SAGE-RL 调整后的模型将推理延迟降低了 40% 以上。
       *   **对比基准**：优于现有的高效推理基准方法（如 AdaptThink, Efficient Reasoning, GRPO-Lead），在保持高准确率的同时实现了最佳或次佳的 Token 压缩率。


============================================================

## 📄 VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training

- **链接**: https://huggingface.co/papers/2602.10693
- **阅读来源**: HTML

# VESPO: Variational Sequence-Level Soft Policy Optimization 论文报告

1. **应用领域**
   NLP - 大语言模型强化学习（LLM Reinforcement Learning），特别是涉及离策略（Off-Policy）训练、数学推理任务及混合专家模型（MoE）的场景。

2. **一句话核心贡献**
   提出了一种基于变分原理的序列级软策略优化方法（VESPO），通过推导闭式解的权重重塑核，有效解决了大模型在策略滞后、异步训练及训练-推理失配场景下的训练不稳定与方差爆炸问题。

3. **使用指南**
   *   **输入**：
       *   提示词（Prompt）$x$ 和模型生成的完整回复序列（Response）$y$。
       *   序列级奖励信号 $R(\tau)$。
       *   当前策略 $\pi_\theta$ 和行为策略（采样策略）$\mu$ 下每个 Token 的对数概率。
   *   **输出**：
       *   经过加权调整后的策略梯度，用于更新模型参数。
   *   **实现步骤**：
       1.  在对数空间计算序列级重要性采样权重 $\log W$（即生成序列在当前策略与行为策略下的概率比）。
       2.  应用 VESPO 推导出的加权函数 $f(W)$（基于不对称的指数衰减形式）计算梯度缩放系数。
       3.  执行类似 REINFORCE 的梯度更新，无需像 PPO 那样进行 Token 级别的硬截断。
   *   **开源状态**：论文摘要提及代码已公开（Code is available）。
   *   **硬件需求**：通用 GPU 训练环境（实验使用了 NVIDIA H20）。

4. **主要创新点**
   1.  **变分框架下的权重重塑**：将重要性采样权重的设计重新表述为一个带方差约束的变分优化问题，从而推导出具有理论保证的闭式解（Reshaping Kernel），取代了以往方法中缺乏统一理论基础的启发式截断或归一化操作。
   2.  **无长度归一化的序列级优化**：VESPO 直接作用于序列级重要性权重，保留了 Token 间的依赖结构（Product Structure）。与 GSPO 等方法不同，它摒弃了导致“长度偏差”（Length-dependent bias）的长度归一化，避免了模型因错误偏好长序列而导致的训练崩溃。
   3.  **非对称软抑制机制**：提出了一种光滑的软加权函数，随着重要性权重的偏离逐渐衰减而非突然截断。同时，针对正负优势（Advantage）样本采用非对称的超参数控制（对负优势样本施加更强的抑制），有效防止了低质量样本导致的梯度方差爆炸。

5. **实验效果**
   *   **核心数据集**：在数学推理基准数据集 AIME 2024、AIME 2025、AMC 2023 和 MATH-500 上进行了评估。
   *   **模型与环境**：主要基于 Qwen3-30B-A3B（MoE架构）、Llama-3.2-3B 等模型，在高达 64 的策略滞后比率（Staleness Ratio）和完全异步训练设置下测试。
   *   **主要结果**：
       *   **稳定性**：在极端的策略滞后（gbs/mbs=64）和异步训练中，VESPO 始终保持稳定的训练曲线，而基线方法（如 GRPO、GSPO、SAPO）则出现性能饱和或训练崩溃。
       *   **准确率**：在 Qwen3-30B MoE 模型上，VESPO 取得了所有方法中的最高平均准确率，优于最佳基线 2.3%。
       *   **鲁棒性**：在面临训练与推理引擎计算不一致（Train-Inference Mismatch）时，VESPO 展现出优异的鲁棒性，结合 R2（Router Replay）技术后取得了最佳的训练奖励和基准测试分数。


============================================================
