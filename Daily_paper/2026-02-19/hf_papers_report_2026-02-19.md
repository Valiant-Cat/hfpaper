# Hugging Face Daily Papers Report
**Date**: 2026-02-19
**Source URL**: https://huggingface.co/papers/date/2026-02-19

============================================================

## 📄 Visual Memory Injection Attacks for Multi-Turn Conversations

- **链接**: https://huggingface.co/papers/2602.15927
- **阅读来源**: HTML

1. **应用领域**：
   多模态大模型安全（Multimodal LLM Security）、对抗性攻击（Adversarial Attacks）、视觉-语言模型鲁棒性（Vision-Language Model Robustness）。

2. **一句话核心贡献**：
   提出了一种名为“视觉记忆注入”（Visual Memory Injection）的隐蔽攻击方法，通过向图像添加对抗扰动，使多模态大模型在长多轮对话中表现正常，但一旦用户提及特定触发话题，模型即会输出攻击者预设的恶意内容（如虚假推荐或政治引导）。

3. **使用指南**：
   *   **输入**：一张原始图像、一个良性锚定提示词（Anchor Prompt）、一个触发提示词（Trigger Prompt）以及攻击者希望模型输出的目标响应（Target Response）。
   *   **过程**：使用白盒梯度优化算法（如APGD），在限制像素扰动范围（$\epsilon$）内对图像进行优化。优化过程中需应用“上下文循环”（Context-Cycling）和“良性锚定”（Benign Anchoring）技术。
   *   **输出**：一张带有不可察觉扰动的对抗图像。
   *   **部署**：攻击者将图片发布到网络，受害用户下载并将其用于LVLM对话。无需特殊硬件即可推理，但在生成对抗样本时需要GPU资源。源码已在GitHub公开。

4. **主要创新点**：
   *   **针对多轮对话的持久化攻击**：不同于以往仅针对单轮交互的攻击，该研究利用了图像在LVLM上下文中持续存在的特性，实现了在长达25轮以上的无关对话后仍能精准触发的攻击，显著扩展了攻击的时间跨度和隐蔽性。
   *   **上下文循环优化策略（Context-Cycling）**：提出了一种在优化过程中动态变化对话历史长度（从短到长循环）的训练策略，强制对抗扰动适应不同长度和内容的对话上下文，从而解决了攻击难以在变长对话中泛化的问题。
   *   **双重目标隐蔽机制**：结合了“良性锚定”与“目标触发”的双重优化目标。既强制模型在首轮及非触发话题下输出正常、有帮助的回复（防止模型退化引起怀疑），又确保在出现特定关键词时输出预设恶意内容，实现了极高的攻击隐蔽性。

5. **实验效果**：
   *   **攻击成功率高**：在 Qwen-VL、LLaVA-OneVision 等主流开源模型上，针对股票推荐、产品营销和政治引导等多种场景，该攻击在多轮对话后仍保持极高的攻击成功率（Attack Success Rate）。
   *   **迁移性与泛化性强**：实验证明，生成的对抗图像不仅对未见过的对话上下文（Unseen Contexts）和转述后的提示词（Paraphrased Prompts）有效，还能直接迁移攻击到基于源模型微调的第三方模型（如从 Qwen3-VL 迁移攻击 SEA-LION 和 Med3 模型）。
   *   **长文本鲁棒性**：在包含超过10,000个token的长对话历史中，攻击依然有效，且未在非触发轮次中发生目标泄漏（Leakage）。


============================================================

## 📄 Learning Situated Awareness in the Real World

- **链接**: https://huggingface.co/papers/2602.16682
- **阅读来源**: ArXiv Abs

# 论文阅读报告：Learning Situated Awareness in the Real World

1. **应用领域**
   计算机视觉 - 第一人称视频理解 (Egocentric Video Understanding) / 多模态大模型评估 (Multimodal Foundation Models Evaluation) / 具身智能 (Embodied AI)

2. **一句话核心贡献**
   提出了 SAW-Bench 基准测试集，专门用于评估多模态大模型在真实世界中基于观察者视角（Observer-centric）的空间感知、姿态理解及动作推理能力，填补了以往基准仅关注环境中心关系的空白。

3. **使用指南**
   *   **输入**：由智能眼镜（如 Ray-Ban Meta Gen 2）采集的真实世界第一人称视角视频。
   *   **输出**：针对特定感知任务的文本问答（QA），涉及对自身在环境中的位置、可能的动作及空间关系的推理。
   *   **硬件/数据**：数据集包含 786 个视频和 2071 个问答对；使用时主要作为测试集输入给多模态大模型（MFMs）。
   *   **代码/数据状态**：文中提及引入了该 benchmark，通常意味着相关数据集和评估脚本将开源供社区使用。

4. **主要创新点**
   *   **评估范式转移**：区别于传统关注场景内物体间关系（环境中心）的基准，本研究通过“观察者中心”视角，强调相对于 Agent 视点、姿态和运动的空间推理。
   *   **构建 SAW-Bench 数据集**：收集了覆盖多种室内外环境的 786 个自录视频和 2071 个高质量人工标注，设计了 6 种不同的情境感知任务（Awareness Tasks）。
   *   **揭示模型几何推理缺陷**：研究发现模型虽然能利用部分视觉几何线索，但普遍缺乏连贯的相机几何（Camera Geometry）构建能力，导致系统性的空间推理错误。

5. **实验效果**
   *   **人机差距显著**：在 SAW-Bench 上的综合评估显示，即使是表现最好的多模态大模型（文中指 Gemini 3 Flash），其性能仍落后人类 **37.66%**。
   *   **主要发现**：现有模型难以从被动观察跨越到理解物理接地的、以观察者为中心的动态场景，证明了该基准在衡量“情境空间智能”（Situated Spatial Intelligence）方面的挑战性和有效性。


============================================================

## 📄 RynnBrain: Open Embodied Foundation Models

- **链接**: https://huggingface.co/papers/2602.14979
- **阅读来源**: HTML

### 1. 应用领域
**具身智能 (Embodied AI)**、**多模态大模型 (Multimodal Foundation Models)**、**机器人感知与规划 (Robot Perception & Planning)**、**视觉语言导航 (VLN)**。

### 2. 一句话核心贡献
提出了 RynnBrain，一个开源的具身智能时空基础模型，通过统一架构集成了以自我为中心的感知、时空定位、物理落地推理（Physically Grounded Reasoning）和物理感知规划能力，解决了现有模型缺乏物理世界一致性和细粒度操控规划的问题。

### 3. 使用指南
*   **输入数据**：支持多模态输入，包括全向视觉信号（单视图图像、多视图图像、视频）以及自然语言指令和时空坐标。
*   **输出数据**：联合生成自然语言文本和显式的空间定位图元（Spatial Grounding Primitives），包括坐标点、边界框（Bounding Boxes）和运动轨迹，直接服务于机器人控制系统。
*   **模型规格与硬件**：提供 **2B**、**8B**（密集模型）和 **30B-A3B**（混合专家模型 MoE）三种规模，适配不同的计算资源限制。训练过程利用了 ZeRO 优化器和 DeepSpeed 框架以适应单卡或多卡环境。
*   **开源状态**：代码、模型权重及评测基准（RynnBrain-Bench）均已公开（相关链接见 HuggingFace/ModelScope/GitHub）。

### 4. 主要创新点
1.  **物理落地的统一时空架构**：不同于仅输出文本的传统 VLM，RynnBrain 将高层语义理解与低层物理空间（空间坐标、轨迹）统一编码，使模型具备“时空记忆”和“物理接地”能力，能直接预测物体位置、功能区域（Affordance）及操作轨迹。
2.  **交错式推理机制 (Chain-of-Point, CoP)**：提出了 RynnBrain-CoP 变体，采用文本推理与空间定位交替进行的策略（Interleaved Reasoning），强制模型的推理步骤必须锚定在具体的物理视觉证据上，有效减少了长程规划中的幻觉。
3.  **多样化的后训练体系与数据飞轮**：构建了人机协同的高质量数据构建流程（规模超 2000 万样本），并针对下游任务开发了四种后训练变体（CoP 推理、Nav 导航、Plan 规划、VLA 动作控制），实现了从通才大脑到专用技能的有效迁移。

### 5. 实验效果
*   **综合性能**：在 20 个具身智能基准和 8 个通用视觉理解基准的广泛评估中，RynnBrain 系列模型大幅优于现有的具身基础模型（如 RoboBrain 等）。
*   **自研基准表现**：在作者提出的 **RynnBrain-Bench**（涵盖 21 项细粒度时空具身能力）上，模型展现了卓越的自我中心认知、OCR 及多维定位能力（物体、区域、功能点、轨迹）。
*   **下游任务**：
    *   **导航**：RynnBrain-Nav 在 R2R 基准上取得了 SOTA 成绩，优于基于 Qwen3-VL 的同类模型。
    *   **推理**：RynnBrain-CoP 的交错式推理范式在复杂时空任务（如轨迹预测）上将性能提升了约 7%。
    *   **操控**：RynnBrain-VLA 在高复杂度抓取场景中，表现优于基于 OpenVLA 微调的模型，验证了其物理感知规划的有效性。


============================================================

## 📄 Multi-agent cooperation through in-context co-player inference

- **链接**: https://huggingface.co/papers/2602.16301
- **阅读来源**: HTML

### 1. 应用领域
多智能体强化学习 (Multi-Agent Reinforcement Learning, MARL)、博弈论 (Game Theory)、序列建模与大模型智能体 (Sequence Modeling & Foundation Model Agents)。

### 2. 一句话核心贡献
本文证明了在无需复杂的元梯度或硬编码假设的情况下，仅需通过针对多样化对手分布训练序列模型智能体，即可自然诱导出上下文学习（In-context Learning）能力，进而通过相互的“勒索”压力演化出稳健的多智能体合作行为。

### 3. 使用指南
*   **输入数据**：智能体的交互历史序列 $x_{\leq t}$，包含观测值 $o$、动作 $a$ 和奖励 $r$。
*   **核心流程**：
    1.  **构建混合对手池**：创建一个包含学习型智能体和多样化固定策略（Tabular）智能体的环境。
    2.  **训练模型 (PPI算法)**：使用本文提出的**预测策略改进 (Predictive Policy Improvement, PPI)** 算法。该算法训练一个序列模型（如 GRU）作为世界模型和策略先验，通过自监督损失（预测动作、观测、奖励）进行优化。
    3.  **策略提升**：在部署或收集数据时，利用序列模型进行蒙特卡洛（Monte Carlo）推演来估计 Q 值，并使用玻尔兹曼分布 $\pi(a|x_{\leq t}) \propto p(a|x_{\leq t})\exp(\beta \hat{Q})$ 选择动作。
*   **硬件与工具**：算法基于 Python 和 Google JAX/Flax 框架实现，通常需要 GPU 进行序列模型的训练。

### 4. 主要创新点
1.  **基于上下文学习的合作涌现机制**：揭示了多样化训练 -> 上下文对手推断 -> 易被勒索性 (Extortability) -> 相互合作的因果链条。证明了序列模型在单次剧集内的快速适应能力可以替代传统方法中复杂的“元学习者”与“朴素学习者”的时间尺度分离，自然地解决社会困境。
2.  **预测策略改进 (PPI) 算法**：提出了一种受最大后验策略优化 (MPO) 启发的新型 RL 算法。PPI 摒弃了独立的值函数网络，而是直接利用自监督训练的生成式序列模型作为世界模型来进行规划 (Planning) 和策略改进，弥合了序列建模与强化学习的鸿沟。
3.  **预测均衡 (Predictive Equilibrium) 理论**：建立了 PPI 算法的理论框架，定义了“预测均衡”概念，并从理论上证明了在拥有完美世界模型的极限情况下，该预测均衡等价于博弈论中的主观嵌入均衡 (Subjective Embedded Equilibrium)。

### 5. 实验效果
*   **核心环境**：迭代囚徒困境 (Iterated Prisoner’s Dilemma, IPD)。
*   **合作表现**：在混合对手池（包含随机参数的 Tabular 智能体）中训练的 PPI 和 A2C 智能体均成功收敛到了相互合作的高收益状态；相比之下，仅针对单一学习型对手训练或直接获取对手 ID 的对照组则收敛于相互背叛。
*   **适应性验证**：实验显示，受训智能体能够在**单次剧集 (Episode) 内**通过前几轮的交互历史，快速推断出对手是某种特定策略（如 Tit-for-Tat）还是随机策略，并迅速调整为最佳响应策略。
*   **机制验证**：分步分析证实，针对固定上下文学习者 (Fixed-ICL) 的训练会产生勒索策略，而两个勒索策略的相互作用最终导致了合作的形成。


============================================================

## 📄 SAM 3D Body: Robust Full-Body Human Mesh Recovery

- **链接**: https://huggingface.co/papers/2602.15989
- **阅读来源**: ArXiv Abs

# SAM 3D Body (3DB) 论文研究报告

### 1. 应用领域
**计算机视觉 - 3D人体网格重建 (3D Human Mesh Recovery, HMR)**

### 2. 一句话核心贡献
提出了一种名为 SAM 3D Body (3DB) 的可提示（Promptable）模型，通过引入解耦骨骼与形状的新型 MHR 表示法以及高质量数据引擎，显著提升了单张图像全全身（含手部和脚部）3D 重建在复杂野外场景下的鲁棒性和泛化能力。

### 3. 使用指南
*   **输入**：单张 RGB 图像；支持可选的辅助提示输入（如 2D 关键点或掩膜 Mask）以引导推理。
*   **输出**：包含身体、脚部和手部姿态的高精度 3D 人体网格。
*   **开源状态**：模型（3DB）和参数化表示方法（MHR）均已开源。
*   **交互方式**：采用类似 SAM（Segment Anything Model）的交互模式，允许用户通过提示来优化输出结果。

### 4. 主要创新点
1.  **新型参数化表示 Momentum Human Rig (MHR)**：首次采用 MHR 参数化网格表示，成功将人体骨骼结构与表面形状进行解耦，相比传统表示法更利于精准建模。
2.  **可提示的交互式架构**：设计了编码器-解码器架构，支持集成 2D 关键点和掩膜等辅助提示，实现了类似于 SAM 系列模型的用户引导式推理，增强了模型的可控性。
3.  **鲁棒的数据生成引擎**：构建了包含手动标注、微分优化、多视图几何和密集关键点检测的多阶段标注流水线，专门针对稀有姿态和罕见成像条件筛选和处理数据，极大丰富了训练数据的多样性。

### 5. 实验效果
*   **SOTA 表现**：在定性用户偏好研究和传统定量分析指标上，均表现出显著优于现有方法的性能，达到了最先进（State-of-the-art）水平。
*   **强泛化能力**：在多样化的野外（in-the-wild）条件下展现出卓越的泛化能力和一致的准确性。
*   **新基准测试**：论文提出了一个新的按姿态和外观分类组织的评估数据集，验证了模型在细粒度分析下的优越表现。


============================================================

## 📄 Towards a Science of AI Agent Reliability

- **链接**: https://huggingface.co/papers/2602.16666
- **阅读来源**: HTML

AI 分析出错: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later.', 'status': 'UNAVAILABLE'}}


============================================================

## 📄 MMA: Multimodal Memory Agent

- **链接**: https://huggingface.co/papers/2602.16493
- **阅读来源**: HTML

# MMA: Multimodal Memory Agent 研究报告

1. **应用领域**
   多模态大模型智能体（Multimodal LLM Agents）、长上下文推理（Long-context Reasoning）、检索增强生成（RAG）、可信人工智能（Trustworthy AI）。

2. **一句话核心贡献**
   提出了一种多模态记忆智能体（MMA），通过引入基于源可信度、时间衰减和共识机制的动态置信度评分，解决了长程任务中因检索到过时、低信度或冲突信息而导致的智能体过度自信和幻觉问题。

3. **使用指南**
   *   **输入**：包含文本和图像的长程交互历史（记忆流）、用户查询。
   *   **核心流程**：
     1.  **检索**：基于相似度检索相关记忆项。
     2.  **置信度评分**：在推理阶段，对每个检索到的记忆项计算可靠性分数 $\mathcal{C}(M_i)$，该分数由三部分加权组成：
         *   **Source (源可信度)**：基于预定义的说话人可信度先验。
         *   **Time (时间衰减)**：基于信息老化程度的指数衰减。
         *   **Consensus (共识)**：基于记忆项在语义邻域内的支持度（网络共识）。
     3.  **决策**：利用计算出的置信度重加权证据，决定是回答、对冲（hedge）还是因证据不足而拒绝回答（abstain）。
   *   **输出**：经过置信度校准的响应或“未知”标签，以及风险调整后的评分。

4. **主要创新点**
   *   **动态记忆置信度评分框架**：不同于传统仅依赖语义相似度的检索，MMA 构建了一个推理时的元认知层，通过融合源可信度、时间衰减和跨记忆一致性来动态评估记忆的可靠性，有效过滤噪声和过时信息。
   *   **MMA-Bench 诊断基准**：设计了一个程序化生成的基准测试，能够精确控制信息源的可靠性先验和结构化的“文本-视觉”冲突，专门用于压力测试智能体在信念修正（Belief Revision）和认知冲突下的表现。
   *   **揭示并缓解“视觉安慰剂效应”**：研究发现 RAG 智能体容易继承基础模型的视觉偏见（即模糊的视觉输入会诱导不合理的确定性），MMA 通过置信度模块有效抑制了这种效应，恢复了智能体在冲突场景下的判断力。

5. **实验效果**
   *   **FEVER (事实验证)**：MMA 在保持基准准确率（约 59.9%）的同时，将不同随机种子间的性能标准差降低了 **35.2%**，并显著提升了基于拒绝回答机制的选择性评分（Selective Score）。
   *   **LoCoMo (长程记忆)**：在信息稀疏的长对话场景中，MMA 的安全导向配置（Source + Time）实现了比基准更高的可执行准确率（Actionable Accuracy），并减少了错误回答的数量。
   *   **MMA-Bench (对抗性冲突)**：在极具挑战性的“可靠性反转”（Type-B）视觉模式下，MMA 达到了 **41.18%** 的准确率，而同等协议下的基准模型（MIRIX）准确率崩塌为 **0.0%**；证明了 MMA 在高噪声和模态冲突环境下的鲁棒性。


============================================================

## 📄 Optimizing Few-Step Generation with Adaptive Matching Distillation

- **链接**: https://huggingface.co/papers/2602.07345
- **阅读来源**: HTML

1. **应用领域**：
生成式 AI - 图像与视频生成（具体涉及扩散模型加速、分布匹配蒸馏、少步生成优化）。

2. **一句话核心贡献**：
提出了一种自校正蒸馏框架 AMD（Adaptive Matching Distillation），通过利用奖励模型作为诊断代理来识别优化中的“禁区（Forbidden Zones）”，并自适应地调整实/虚教师模型的梯度分量，有效解决了少步生成中的训练崩溃问题并显著提升了生成保真度。

3. **使用指南**：
*   **输入**：文本提示词（Text Prompts）或类别标签。
*   **输出**：高质量的图像或视频（仅需极少步推理，如 4 步）。
*   **流程**：
    1.  准备预训练的教师模型（如 SDXL, Wan2.1）和奖励模型（如 ImageReward, VideoAlign）。
    2.  初始化学生模型。
    3.  使用 AMD 框架进行微调蒸馏：计算生成样本的奖励优势（Advantage），据此动态调整实教师（Real Teacher）的吸引力与虚教师（Fake Teacher）的排斥力权重。
*   **硬件要求**：训练过程计算密集，文中实验提及使用了 NVIDIA H100/H800 GPU 集群。
*   **代码/模型**：基于开源模型（如 Wan2.1, SDXL）构建，论文提及将发布相关基准测试，具体 AMD 训练代码通常随论文发表开源。

4. **主要创新点**：
*   **统一的优化视角与“禁区”定义**：首次将多种 DMD（分布匹配蒸馏）变体重新解释为规避“禁区”（即实教师指导不可靠且虚教师排斥力不足的区域）的隐式策略，并建立了一个统一的优化框架来分析这些故障模式。
*   **动态分数自适应（Dynamic Score Adaptation）**：利用奖励模型作为代理，将梯度分解为“分布匹配项”和“条件对齐项”。根据样本质量动态调整两者权重：在低质量区域优先增强排斥力以逃离模式崩溃，在高质量区域增强吸引力以细化生成。
*   **排斥性景观锐化（Repulsive Landscape Sharpening）**：改进了虚教师（Fake Teacher）的训练目标，使其不再均匀地建模学生分布，而是专注于识别和“惩罚”低质量的失败样本，从而构建陡峭的能量壁垒，提供更强的梯度动力将学生模型推回有效数据流形。

5. **实验效果**：
*   **图像生成（SDXL）**：在 MS-COCO 和 GenEval 基准上表现优异。例如，AMD 将 HPSv2 得分从 DMD2 的 **28.52 提升至 31.25**，ImageReward 得分达到 **88.37**，显著超越了 PCM、DMD2 等 SOTA 方法。
*   **视频生成（Wan2.1）**：在 VBench 基准测试中，AMD 在总分（Total Score）和运动质量（Motion Quality）上均超越了 LongLive 基准，展示了更好的时序一致性和动态保真度。
*   **训练稳定性**：在 SiT-XL/2 模型上的实验表明，AMD 有效避免了传统 DMD 方法中的奖励破解（Reward Hacking）和模式崩溃问题，实现了 FID 和 IS 指标的同步提升。


============================================================

## 📄 Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation

- **链接**: https://huggingface.co/papers/2602.16705
- **阅读来源**: ArXiv Abs

# 论文阅读报告：Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation

### 1. 应用领域
机器人学（Robotics）- 人形机器人移动操控（Visual Loco-Manipulation）/ 具身智能（Embodied AI）

### 2. 一句话核心贡献
提出了一种名为 HERO 的新范式，通过结合大视觉模型的开放词汇理解能力与仿真训练的高精度残差感知控制策略，解决了人形机器人在非结构化真实环境中对任意物体进行泛化移动操控的难题。

### 3. 使用指南
*   **输入**：视觉传感器数据（如 RGB-D 图像）以及针对特定物体的开放词汇（Open-Vocabulary）指令。
*   **处理流程**：
    1.  利用大视觉模型（Large Vision Models）进行场景理解和目标定位，实现强泛化能力。
    2.  通过模块化系统调用“残差感知末端执行器（EE）跟踪策略”进行动作规划。
*   **输出**：人形机器人的末端执行器精准轨迹及全身协调控制指令。
*   **硬件要求**：配备 RGB-D 相机的人形机器人平台。

### 4. 主要创新点
1.  **HERO 混合范式**：创造性地将大视觉模型的强泛化能力（用于感知）与基于仿真训练的强控制性能（用于动作）相结合，克服了传统模仿学习因数据匮乏导致的泛化性差的问题。
2.  **残差感知 EE 跟踪策略**：设计了一套高精度的末端执行器跟踪系统，该系统融合了经典机器人学方法（逆运动学 IK）与机器学习技术（神经前向模型、目标调整与重规划），实现了从粗糙指令到精准执行的转换。
3.  **神经前向模型校准**：引入学习型的神经前向模型来辅助经典逆运动学，用于精确计算前向运动学并生成参考轨迹，有效弥补了传统模型在复杂人形机器人控制中的误差。

### 5. 实验效果
*   **控制精度**：相比现有基准方法，所提出的技术创新将末端执行器的跟踪误差降低了 **3.2倍**。
*   **真实场景泛化**：系统在办公室、咖啡店等多样化的真实世界环境中表现稳健。
*   **任务适应性**：能够可靠地操控各种日常物品（如杯子、苹果、玩具），并能适应高度范围在 **43cm 至 92cm** 之间的不同操作平面，在仿真和真机实验中均证明了其有效性。


============================================================

## 📄 Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality

- **链接**: https://huggingface.co/papers/2602.14080
- **阅读来源**: ArXiv Abs

# 论文阅读报告：Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality

1. **应用领域**
   NLP-大模型评估与知识表示（Large Language Model Evaluation & Knowledge Representation）

2. **一句话核心贡献**
   提出了一种区分“知识未存储”（空货架）与“知识无法提取”（丢钥匙）的行为分析框架，揭示了对于前沿大模型而言，事实性错误的主要瓶颈在于**知识调取（Recall）**而非知识编码（Encoding）。

3. **使用指南**
   *   **输入**：特定的事实性知识点或查询问题。
   *   **方法**：利用论文提出的行为框架，通过不同层级的提示（直接询问 vs. 引导推理/思考）对模型进行测试。
   *   **输出**：将该知识点在模型中的状态分类为以下三种之一：未编码（Not Encoded）、可直接调取（Directly Recalled）、需推理计算调取（Recalled with Inference-time Computation）。
   *   **资源**：该方法依赖于论文构建的 WikiProfile 基准测试流程（由基于搜索的大模型自动构建），无需特殊硬件，主要基于标准模型推理接口进行。

4. **主要创新点**
   *   **细粒度评估框架**：打破了传统评估将所有事实性错误一视同仁的做法，将评估颗粒度从“问题级”深入到“事实级”，明确区分了知识的**存储状态**与**访问状态**。
   *   **WikiProfile 基准**：构建了一个通过基于网络搜索（Web Search）的大模型自动生成的全新基准测试集，专门用于支持这种深度的知识画像分析。
   *   **引入“思考”机制作为补救手段**：明确提出推理时计算（Inference-time computation/Thinking）可以作为一种机制，帮助模型“找回”那些已编码但暂时无法直接调取的“丢失”知识。

5. **实验效果**
   基于 WikiProfile 基准，对 13 个大模型（涵盖前沿模型）的 400 万次响应进行了分析，主要发现如下：
   *   **编码接近饱和**：在前沿模型（文中提及 GPT-5 和 Gemini-3）中，事实知识的编码率已高达 **95-98%**，说明模型已经“记住”了绝大多数知识。
   *   **调取是瓶颈**：许多以往被归因为“知识缺失”的错误，实际上是**调取失败**。这种失败具有系统性，且不成比例地影响**长尾知识**和**反向问题**（Reverse Questions）。
   *   **推理提升效果**：实验证明，通过增加推理时的计算量（Thinking），可以恢复大量因调取失败而导致的错误，表明未来的性能提升将更多依赖于如何利用已有知识，而非单纯扩大规模。


============================================================

## 📄 SLA2: Sparse-Linear Attention with Learnable Routing and QAT

- **链接**: https://huggingface.co/papers/2602.12675
- **阅读来源**: HTML

1. **应用领域**：计算机视觉 - 视频生成大模型加速（特别是基于Transformer的视频扩散模型，如Wan2.1）。

2. **一句话核心贡献**：提出了一种名为SLA2的稀疏-线性注意力机制，通过引入可学习的动态路由、修正的数学分解公式以及量化感知训练（QAT），解决了传统SLA方法中启发式分流不优和量化误差大的问题，显著提升了视频生成的推理速度而不损失质量。

3. **使用指南**：
    *   **输入/输出**：输入为模型中间层的Query、Key、Value张量；输出为注意力计算后的特征图。
    *   **集成方式**：作为插件替换现有的全注意力（Full Attention）或SLA模块。推理阶段支持低比特（Low-bit）计算。
    *   **训练要求**：需要两阶段微调。第一阶段初始化可学习路由（Router），第二阶段进行端到端微调（使用SoftTop-k处理梯度不可导问题）。
    *   **硬件支持**：算法基于FlashAttention内核实现，专为GPU加速设计，利用低精度计算优势。

4. **主要创新点**：
    1.  **可学习的动态路由与混合公式**：通过引入可学习的投影层（$\text{proj}_q, \text{proj}_k$）代替基于幅度的启发式选择，动态决定哪些token进入稀疏分支或线性分支；同时提出了数学上严谨的混合公式 $O = \alpha \odot O_s + (1-\alpha) \odot O_l$，修正了原始SLA中的缩放误差。
    2.  **量化感知注意力（QAT）**：在稀疏注意力分支集成了低比特（Low-bit）设计，并在训练过程中模拟量化噪声（前向低比特，反向FP16），使模型能适应量化误差，从而在推理时安全地使用低精度计算进行加速。
    3.  **SoftTop-k 训练策略**：为了解决Top-k操作不可导的问题，训练期间使用基于重参数化技巧的 SoftTop-k 算子进行梯度回传，而推理时切换回 Hard Top-k，保证了训练的可行性和推理的高效性。

5. **实验效果**：
    *   **核心模型**：在 Wan2.1-1.3B 和 Wan2.1-14B 视频扩散模型上进行了测试。
    *   **加速效果**：实现了 **97%** 的注意力稀疏度，注意力模块计算加速高达 **18.6倍**。在1.3B模型上，端到端延迟大幅降低（例如从97秒降至7秒）。
    *   **生成质量**：在 VBench 和 Vision Reward 等指标评估中，SLA2 优于 VMoBA、VSA 等SOTA稀疏注意力方法；且经过微调后，其生成质量甚至超越了原始的全注意力（Full Attention）基线。


============================================================

## 📄 BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models

- **链接**: https://huggingface.co/papers/2602.08392
- **阅读来源**: HTML

1. **应用领域**：
具身智能 (Embodied AI)、多模态大语言模型 (MLLMs)、机器人双臂操作 (Bimanual Robotic Manipulation)。

2. **一句话核心贡献**：
提出了首个专门用于评估多模态大语言模型在双臂机器人操作中时空协调能力的分层基准测试框架 BiManiBench，填补了现有基准仅关注单臂操作且无法区分感知幻觉与规划失败的空白。

3. **使用指南**：
*   **输入数据**：智能体接收当前步骤的图像（包括第一人称和可能的第三人称视角以解决遮挡）、语言指令、环境交互历史记录以及任务特定的辅助信息。
*   **输出形式**：模型需输出结构化的 JSON 格式数据。根据评估层级不同，输出内容为：
    *   空间推理层：左右臂的选择决策；
    *   高层规划层：原子动作原语序列（如 Pick, Place）及其参数；
    *   低层控制层：双臂各 8 维（共 16 维）的连续控制向量（含末端执行器位姿和夹爪状态）。
*   **运行机制**：框架采用“动作分块”（Action Chunking）技术进行多步预测，并结合“任务自适应执行截断”（Task-Adaptive Execution Truncation）机制，在检测到运动学约束冲突或达到特定步数时中断执行并重新规划，以缓解动作滞后问题。
*   **仿真环境**：基于 SAPIEN 仿真引擎，利用 RoboTwin 提供的高质量任务资产。

4. **主要创新点**：
*   **分层评估体系**：设计了包含三个层级的评估框架——基础空间推理（Spatial Reasoning）、高层动作规划（High-Level Action Planning）和低层末端执行器控制（Low-Level End-Effector Control），能够系统地分离并诊断模型在空间感知、逻辑规划和精细控制方面的具体缺陷。
*   **双臂协调分类学**：定义了三种双臂协调模式：独立并行操作（两臂在同一工作空间但不共享对象）、同步协作操作（两臂同时操作同一对象，如抬重物）和顺序协作操作（存在严格时间依赖，如双手交接），以测试模型处理空间冲突和时间同步的能力。
*   **鲁棒的评估协议**：引入了基于高斯加权的空间评分标准（解决中心区域的左右臂判定模糊性）和带有反馈截断机制的执行管道，有效区分了因感知幻觉导致的失败和因规划逻辑错误导致的失败。

5. **实验效果**：
在对超过 30 个最先进的 MLLM（包括 GPT-5, Gemini-2.5-Pro, Claude-3.5-Sonnet, InternVL3, Qwen2.5-VL 等）进行评估后发现：
*   **闭源模型优势明显**：在所有层级中，闭源模型（尤其是 GPT-5 和 Gemini-2.5-Pro）表现最佳。在低层控制任务中，GPT-5 达到 66.80% 的成功率，而最强的开源模型（InternVL3-78B）仅为 36.60%，差距显著。
*   **“控制”落后于“规划”**：大多数模型在高层语义规划上表现尚可，但在结合视觉反馈的精细连续控制和双臂空间定位上存在严重困难，常导致碰撞或次序错误。
*   **多视角信息的双刃剑效应**：虽然引入第三人称视角能帮助高性能模型解决遮挡问题，但对于推理能力有限的小型模型，额外视角反而构成了信息过载（Visual Noise），导致成功率下降。


============================================================

## 📄 World Action Models are Zero-shot Policies

- **链接**: https://huggingface.co/papers/2602.15922
- **阅读来源**: HTML

1. **应用领域**：
具身智能 (Embodied AI)、机器人操作 (Robot Manipulation)、世界模型 (World Models)、视觉-语言-动作模型 (VLA)。

2. **一句话核心贡献**：
提出了 DreamZero（一种基于预训练视频扩散模型的 14B 参数世界动作模型 WAM），通过联合预测未来视频帧和动作，实现了在异构数据上的有效学习、强大的零样本任务泛化能力以及实时的闭环控制（7Hz）。

3. **使用指南**：
*   **输入**：视觉上下文（当前及历史观察帧）、自然语言指令、机器人本体状态（proprioceptive state）。
*   **输出**：未来的视频帧序列（作为视觉规划）和相应的动作块（Action Chunks，用于控制机器人）。
*   **模型架构**：基于 Flow Matching 的自回归 DiT（Diffusion Transformer）骨干网络。
*   **硬件要求**：由于模型规模达到 14B 且涉及视频生成，推理对算力要求较高。文中在 NVIDIA GB200/Blackwell 架构上进行了优化测试，实现了 150ms 的延迟；普通推理可能需要多 GPU 并行。
*   **代码与资源**：作者计划开源模型权重、推理代码以及相关基准测试代码（PolaRiS 和 Genie Sim 3.0）。

4. **主要创新点**：
*   **联合视动预测的世界动作模型（WAM）**：不同于传统的 VLA 直接预测动作，该方法利用预训练视频扩散模型的时空先验，联合生成视频和动作。视频预测充当隐式视觉规划器，显著提升了从非重复、多样化数据中学习的能力，实现了零样本泛化。
*   **DreamZero-Flash 推理加速机制**：提出了一种解耦的噪声调度策略（Decoupled Noise Schedules），在训练中让视频保持高噪声而动作保持低噪声，从而使得推理时仅需单步去噪即可生成高质量动作，配合 KV Cache 和系统级优化（量化、算子融合），将 14B 模型的推理速度提升了 38 倍（从 5.7秒降至 150毫秒），实现了 7Hz 的实时控制。
*   **高效的跨具身迁移与适应**：展示了仅利用无动作标注的视频数据（如人类或其他机器人的视频）即可提升 42% 以上的任务性能；且仅需 30 分钟的“玩耍数据”即可将模型适配到全新的机器人形态上，同时保留零样本泛化能力。

5. **实验效果**：
*   **零样本泛化**：在真实机器人（AgiBot G1 和 Franka）实验中，相比 SOTA VLA 模型（如 OpenVLA, GR00T），在未见过的任务和环境中平均任务进度（Task Progress）提升了 **2倍以上**。
*   **数据效率**：证明了模型可以从 500 小时多样化、非重复的异构数据中有效学习，打破了通才策略依赖重复演示数据的传统认知。
*   **跨具身迁移**：在从 AgiBot 到 YAM 机器人的迁移实验中，利用人类视频（12分钟）或其他机器人视频（20分钟）作为辅助，在未见任务上取得了显著性能提升。
*   **微调性能**：在特定任务微调后，性能比 SOTA VLA 模型高出 10%。


============================================================
