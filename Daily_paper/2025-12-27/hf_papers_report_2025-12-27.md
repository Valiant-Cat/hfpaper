# Hugging Face Daily Papers Report
**Date**: 2025-12-27
**Source URL**: https://huggingface.co/papers/date/2025-12-27

============================================================

## 📄 VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation

- **链接**: https://huggingface.co/papers/2512.19680
- **阅读来源**: HTML

# VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation

1. **应用领域**：
   计算机视觉 - 自回归图像生成 (Autoregressive Image Generation)、多模态生成模型微调 (如 LlamaGen, Janus-Pro)。

2. **一句话核心贡献**：
   提出了一种基于变分原理的轻量级强化学习后训练框架 VA-π，通过将像素级重建质量作为内在奖励直接优化自回归模型，有效解决了离散 Token 预测与像素空间分布不一致的问题，显著提升了生成图像的视觉保真度。

3. **使用指南**：
   *   **输入**：预训练的自回归 (AR) 生成器（如 LlamaGen、Janus-Pro）和少量参考图像数据（仅需预训练数据的约 1%）。
   *   **流程**：该方法作为一个高效的后训练（Post-training）阶段。它不需要重新训练 Tokenizer，也不需要额外的外部奖励模型（Reward Model）。
   *   **计算需求**：极其高效。例如，在 LlamaGen-XXL 模型上，使用 8 张 A100 GPU 仅需微调 25 分钟。
   *   **代码获取**：论文提到代码已开源。

4. **主要创新点**：
   *   **变分对齐目标 (Variational Alignment Objective)**：推导出一个基于证据下界 (ELBO) 的优化目标，将像素空间的重建误差转化为强化学习中的内在奖励，并将 AR 模型的似然目标作为先验正则化项，从理论上统一了离散 Token 建模与像素级生成。
   *   **高效的策略优化机制**：不同于传统的需要昂贵“自由运行（Free-running）”采样的 RL 方法，VA-π 利用 Teacher Forcing 和上下文噪声注入来计算 logits 并采样目标 Token，直接根据这些 Token 解码后的重建质量进行策略更新（类似 GRPO），大幅降低了计算成本。
   *   **无外部奖励模型的强化学习**：摒弃了对复杂外部奖励模型或人类反馈数据的依赖，直接利用 Tokenizer 的解码重建误差作为反馈信号，通过强化学习将梯度传播到 AR 生成器，解决了离散采样不可导的问题。

5. **实验效果**：
   *   **类别条件图像生成 (ImageNet-1K)**：在 LlamaGen-XXL (1.4B) 模型上，仅使用 1% 的数据和 25 分钟微调，在无分类器指导 (CFG) 的情况下，**FID 从 14.36 降至 7.65**（越低越好），**Inception Score (IS) 从 86.55 提升至 116.70**。
   *   **文生图 (Text-to-Image)**：在 GenEval 基准测试中，VA-π 提升了 LlamaGen-XL 和多模态模型 Janus-Pro 的生成质量。例如，Janus-Pro 的总体评分从 0.725 提升至 0.744，在对象计数、属性绑定等复杂任务上表现出明显优势。
   *   **对比优势**：在同等计算预算下，性能显著优于传统的 STE (Straight-Through Estimator) 微调方法和依赖外部奖励模型的 AR-GRPO 方法。


============================================================

## 📄 Schoenfeld's Anatomy of Mathematical Reasoning by Language Models

- **链接**: https://huggingface.co/papers/2512.19995
- **阅读来源**: HTML

### 1. 应用领域
自然语言处理 (NLP) - 大语言模型推理 (LLM Reasoning)、思维链分析 (Chain-of-Thought Analysis)、数学问题求解 (Mathematical Problem Solving) 与认知科学交叉研究。

### 2. 一句话核心贡献
提出了一种基于认知科学 Schoenfeld 片段理论的自动化分析框架 ThinkARM，将大语言模型的数学推理过程抽象为 8 种功能性认知片段，从而揭示了推理模型内部的思维结构、正确性相关的动态模式以及不同效率优化方法的行为本质。

### 3. 使用指南
*   **输入**：大语言模型在解决数学问题时生成的完整推理轨迹（Reasoning Traces / Chain-of-Thought）。
*   **核心流程**：
    1.  **分句**：将推理文本分割为句子。
    2.  **自动标注**：利用高能力的 LLM（论文中推荐 GPT-5 或 GPT-4 级别模型）作为标注器。
    3.  **应用指南**：依据论文定义的标注指南（Guidebook），将每个句子分类为 8 个认知片段之一：**Read**（读题）、**Analyze**（分析）、**Plan**（计划）、**Implement**（执行）、**Explore**（探索）、**Verify**（验证）、**Monitor**（监控）、**Answer**（作答）。
*   **输出**：结构化的推理片段序列（如 `Read -> Plan -> Implement -> Verify`），以及基于此序列的统计特征（如片段占比、转移矩阵、N-gram 模式）。
*   **资源**：论文附录提供了详细的标注指南（Guidebook）和提示词模板（Prompt Template），代码和数据集通常随论文开源（文中提及构建了人类验证的金标集）。

### 4. 主要创新点
1.  **理论驱动的中间层抽象**：首次将认知科学中用于分析人类数学解题的 **Schoenfeld 片段理论**（Episode Theory）系统性地迁移并扩展（增加 Answer 和 Monitor 类别）到大模型推理分析中，提供了一种超越 token 统计、具有可解释性的中间层级表示。
2.  **可扩展的自动化标注管道 (ThinkARM)**：构建并验证了一套自动化标注流程，通过与人类专家标注的“金标集”对比，证明了前沿 LLM（如 GPT-5）能够以极高的一致性进行大规模的句子级认知片段标注，解决了人工分析难以扩展的问题。
3.  **深层推理行为的诊断性发现**：
    *   **结构差异**：量化了“推理模型”（频繁的探索-验证循环）与“非推理模型”（线性的前馈执行）在思维结构上的本质区别。
    *   **正确性关联**：发现“探索”（Exploration）是关键分支点——正确的推理倾向于将探索转化为监控（Monitor）或重分析，而错误的推理往往在探索后盲目执行。
    *   **效率权衡**：揭示了不同的效率优化方法（如 L1 惩罚或 ThinkPrune）并非均匀压缩长度，而是选择性地抑制了验证（Verify）和监控循环。

### 5. 实验效果
在包含不同模型家族（DeepSeek-R1, Phi-4, Qwen 等）和 Omni-MATH 数据集子集的实验中：
*   **标注准确性**：GPT-5 在人类验证的金标集上展现了最高的标注一致性，确立了自动标注的可靠性。
*   **推理模式识别**：成功识别出推理模型特有的“心跳”模式（即分析、执行、验证的周期性波动）和 U 型验证分布（开头和结尾验证较多），而非推理模型则由“执行”（Implement）片段主导。
*   **正确性预测**：通过 Lasso 逻辑回归分析，发现特定的片段转移模式（如 `Explore -> Monitor`）与最终答案的正确性高度正相关。
*   **效率分析**：对比实验显示，L1 正则化等方法会导致模型推理轨迹中 `Verify` 和 `Monitor` 片段的显著减少（特定 N-gram 模式如验证循环的互信息差异高达 0.37），从而以牺牲特定认知过程为代价换取效率。


============================================================

## 📄 Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

- **链接**: https://huggingface.co/papers/2512.20605
- **阅读来源**: HTML

# 论文报告：Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

## 1. 应用领域
**强化学习 (Reinforcement Learning)**，具体涉及**分层强化学习 (Hierarchical RL)**、**基础模型控制 (Foundation Model Control)** 以及**自回归序列模型 (Autoregressive Sequence Models)** 的决策与规划。

## 2. 一句话核心贡献
提出了一种名为“内部强化学习”（Internal RL）的新范式，通过训练一个元控制器（Metacontroller）来操纵冻结的预训练自回归模型的内部残差流激活，从而在无监督情况下发现时间抽象动作，有效解决了稀疏奖励下的长程探索与信贷分配难题。

## 3. 使用指南
*   **输入**：
    *   一个在大规模行为数据集上预训练好的自回归序列模型（如 Transformer 或 SSM），该数据集包含观察-动作序列，但不需要奖励或任务标签。
    *   目标任务环境（通常是具有分层结构的稀疏奖励环境）。
*   **流程**：
    1.  **冻结基模型**：保持预训练的自回归模型参数不变。
    2.  **训练元控制器**：训练一个包含编码器-解码器结构的元控制器。它读取基模型的残差流，通过变分推断学习生成潜在变量（Latent Codes），并解码为线性控制器来干预基模型的中间层激活。此过程利用未来信息进行无监督训练，以发现时间抽象。
    3.  **执行内部 RL**：将基模型和元控制器视为环境的一部分。通过强化学习算法（如改进版的 PPO/GRPO）直接优化元控制器的潜在空间输入，而不是直接优化原始动作输出。
*   **输出**：一个能够执行长时间跨度、组合性任务的智能体策略。
*   **硬件/代码**：论文使用了 Transformer 和 Hawk (SSM) 架构，实验基于 GPU 进行。

## 4. 主要创新点
1.  **内部强化学习 (Internal RL) 机制**：
    区别于传统的在输出层（Logits）进行微调，本文提出在模型的**内部表示空间（残差流）**进行干预和强化学习。通过在抽象的潜在空间中进行探索，将原本需要数百万步才能获得奖励的低效探索（Token-level），转变为高效的高层策略搜索，极大提升了稀疏奖励任务的成功率。

2.  **无监督的时间抽象与切换发现**：
    设计了一种特殊的**元控制器 (Metacontroller)** 架构，结合了非因果（未来条件）编码器和动态切换单元（Switching Unit）。该架构能够在没有任何人工标注的情况下，自动从连续的行为流中“分割”出具有语义意义的时间抽象动作（如“去红色目标”），并学习到稀疏的切换时机。

3.  **冻结基模型的控制策略**：
    研究发现并验证了**冻结**预训练自回归模型对于发现有效的时间抽象至关重要。与联合训练（Co-training）相比，控制一个冻结的、已具备一定预测能力的基模型，能产生更具解耦性和组合性的抽象动作表征，避免了表征坍塌问题。

## 5. 实验效果
*   **数据集/环境**：
    *   **Grid World (Pinpad)**：离散状态空间的导航任务，需按特定顺序访问颜色目标。
    *   **MuJoCo Ant (Continuous Control)**：高维连续控制环境（四足机器人），同样需执行分层导航任务。
    *   任务设定为极具挑战性的**稀疏奖励（Sparse Reward）**设置，仅在完成整个序列目标后给予奖励。

*   **核心表现**：
    *   **对比基线**：与标准 RL 微调（使用 GRPO 算法）、CompILE（一种先前的分层 RL 方法）以及联合训练模型相比。
    *   **成功率**：在 MuJoCo Ant 的分层任务中，标准 RL 微调和 CompILE 的成功率几乎为 **0%**（无法获得奖励信号）。而 Internal RL 方法在数个种子实验中达到了 **80%-100%** 的高成功率。
    *   **泛化性**：实验表明，学习到的内部控制器具有组合泛化能力（Compositional Generalization），能够通过重新组合学到的子目标（抽象动作）来解决预训练期间未见过的新任务配置。


============================================================

## 📄 Latent Implicit Visual Reasoning

- **链接**: https://huggingface.co/papers/2512.21218
- **阅读来源**: HTML

### 1. 应用领域
多模态大模型（LMMs）、计算机视觉与自然语言处理交叉领域（Vision-Language）、视觉推理（Visual Reasoning）、视觉问答（Visual Question Answering, VQA）。

### 2. 一句话核心贡献
论文提出了 LIVR (Latent Implicit Visual Reasoning) 方法，通过引入隐向量（Latent Tokens）和视觉瓶颈注意力机制，使多模态大模型在无需边界框或中间图像等显式监督的情况下，能够自主学习并利用对任务关键的隐含视觉抽象表示，从而显著提升复杂视觉任务的推理性能。

### 3. 使用指南
*   **输入**：参考图像（Reference Image）、候选图像或选项集合、以及对应的文本提示（Prompt/Question）。
*   **输出**：文本形式的答案（如选择题的选项 A/B/C/D 或具体的计数数值）。
*   **核心实现步骤**：
    1.  **模型架构调整**：在文本提示序列后追加 $k$ 个随机初始化的可学习隐向量 (Latent Tokens)。
    2.  **两阶段训练**：
        *   **阶段一（视觉瓶颈）**：应用特殊的注意力掩码，强制“答案 Token”只能关注“隐向量”和“提示 Token”，而不能直接关注“图像 Token”。这迫使隐向量必须从图像中提取并编码关键视觉信息。
        *   **阶段二（联合微调）**：恢复标准掩码，允许答案 Token 同时关注原始图像和已经学习到视觉特征的隐向量，进行端到端的联合训练。
    3.  **参数设置**：保持视觉编码器和投影层冻结，仅训练语言模型的 LoRA 参数和新增的隐向量嵌入层。
*   **硬件需求**：实验基于 NVIDIA RTX 6000 Ada GPU 进行，计算量主要取决于所选用的基座大模型（如 Qwen2.5-VL 或 LLaVA）。

### 4. 主要创新点
1.  **隐式视觉表示学习 (Implicit Visual Representation)**：
    打破了以往方法需要显式监督（如标注边界框、分割图或人工设计的中间步骤）的限制。LIVR 允许模型通过端到端的任务目标，自主在隐空间中发现并编码对当前任务最有用的视觉结构（如物体的角度、空间关系等），这些往往是人类难以用自然语言显式定义的。
2.  **视觉瓶颈注意力机制 (Visual Bottleneck Attention Masking)**：
    设计了一种新颖的“瓶颈”掩码策略。在训练初期，切断了答案生成部分与原始视觉输入的直接联系，强制所有视觉信息的传递必须经过少量的隐向量。这种机制有效地增加了模型的“视觉计算”容量，并减少了语言偏见，迫使模型真正去“看”和“理解”图像。
3.  **任务无关且即插即用 (Task-Agnostic Mechanism)**：
    该方法不依赖于特定任务的先验知识或特定的辅助数据结构，具有极强的通用性。无论是单任务微调还是多任务混合训练，LIVR 均能直接适用，且无需为不同任务设计不同的中间监督目标（如有的任务需要深度图，有的需要关键点，LIVR 统一用隐向量解决）。

### 5. 实验效果
*   **核心数据集**：在 9 个感知密集型任务上进行了评估，这些任务改编自 BLINK 基准及 COCO、PixMo、HPatches 等数据集，涵盖计数、拼图 (Jigsaw)、物体定位、语义/功能对应、艺术风格分类等。
*   **性能表现**：
    *   **显著优于 SFT**：在 Qwen2.5-VL-3B 模型上，LIVR 相比直接监督微调 (Direct SFT) 平均准确率提升了 **6.24%**。
    *   **处理复杂任务能力强**：在需要高度视觉抽象的任务上提升巨大，例如在拼图任务上提升 **12%**，在功能对应任务上提升 **13.02%**。
    *   **超越现有 SOTA**：与依赖显式视觉辅助图像的 Mirage 方法对比，LIVR 在无额外监督数据的情况下，在 Jigsaw 和 VSP 任务上分别取得了 **+19.40%** 和 **+20.00%** 的绝对准确率优势。
    *   **多任务泛化**：在 Qwen3-VL-4B 的多任务混合训练设置下，LIVR 在所有 6 个测试任务上均优于基线，证明了隐向量机制在异构任务间共享的有效性。


============================================================

## 📄 GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

- **链接**: https://huggingface.co/papers/2512.13043
- **阅读来源**: HTML

# GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

1. **应用领域**
   多模态智能体（Multimodal Agents）、视觉语言模型微调（VLM Fine-tuning）、多轮强化学习（Multi-turn Reinforcement Learning）。

2. **一句话核心贡献**
   提出了一种高效的强化学习训练框架，通过融合训练过程中的历史模型 Checkpoints 构建“免费”且强大的教师模型，替代昂贵的外部闭源模型（如 GPT-4）进行思维链指导，在大幅降低训练时间和成本的同时实现了 SOTA 性能。

3. **使用指南**
   *   **输入**：包含视觉图像（如环境观测）和文本指令的多模态数据。
   *   **输出**：智能体的决策序列，包含中间推理过程（Thought）和最终动作（Action）。
   *   **硬件需求**：建议使用至少 2 张 NVIDIA GPU（一张用于训练当前 Student 模型，另一张用于部署融合后的 Teacher 模型进行推理）。
   *   **操作流程**：
      1.  初始化 VLM 基座（如 Qwen2.5-VL）。
      2.  在 RL 训练循环中，将产生的历史 Checkpoints 存入缓冲区。
      3.  使用 TIES Merging 技术动态融合历史权重生成 Teacher 模型。
      4.  Student 模型探索环境生成数据，Teacher 模型基于相同输入计算参考 Logits 或推理文本。
      5.  通过最小化反向 KL 散度（Soft Logit Distillation）或 SFT 损失来更新 Student 模型。

4. **主要创新点**
   1.  **自进化的模型融合教师机制（Merged Checkpoint as Teacher）**：发现并利用了 RL 训练中的历史 Checkpoint 融合模型（采用 TIES Merging）比当前单点模型更稳定、更强大的特性，直接将其作为 Teacher，消除了对 GPT-4 等外部昂贵 API 的依赖。
   2.  **基于反向 KL 散度的软 Logit 蒸馏**：提出使用反向 KL 散度（Reverse KL Divergence）作为辅助奖励信号，相比传统的 SFT 硬标签，这种方法利用了 Teacher 的完整概率分布信息，仅需一次前向传播即可计算，效率更高且更能鼓励模型探索。
   3.  **兼顾效率与性能的自举训练范式**：解决了传统 RL（如 RL4VLM）中的“思维坍塌”问题，同时避免了 GTR 方法的高昂成本。相比 GTR，GTR-Turbo 训练时间减少 50%，算力成本降低 60%，且无需任何外部监督信号。

5. **实验效果**
   在两个具有挑战性的视觉智能体基准数据集上验证了有效性（基于 Qwen2.5-VL-7B 等模型）：
   *   **Points24（视觉数学推理）**：GTR-Turbo 取得了 SOTA 性能，超越了使用 GPT-4o 作为教师的原始 GTR 方法以及其他开源/闭源模型，同时实现了训练成本的大幅下降。
   *   **ALFWorld（具身智能决策）**：在长序列、稀疏奖励的家庭环境任务中，GTR-Turbo 展现出快速且稳定的成功率提升，优于同等规模下的多个基线模型，并且在最新的 Qwen3-VL 模型上也展现了良好的兼容性和性能提升。


============================================================

## 📄 How Much 3D Do Video Foundation Models Encode?

- **链接**: https://huggingface.co/papers/2512.19949
- **阅读来源**: HTML

# 论文分析报告：How Much 3D Do Video Foundation Models Encode?

## 1. 应用领域
**计算机视觉** - 视频基础模型分析、3D场景重建、视频生成模型理解与评估。

## 2. 一句话核心贡献
本文提出了首个模型无关的评估框架，通过训练浅层探针（Probe）定量衡量了视频基础模型（VidFMs）的3D感知能力，并揭示了SOTA视频生成模型（如WAN2.1）即使仅在2D数据上训练，也涌现出了超越图像模型甚至3D专家模型的强大全局3D结构理解能力。

## 3. 使用指南
*   **输入**：一段视频剪辑（通常采样4帧作为输入序列）。
*   **流程**：
    1.  **特征提取**：保持视频基础模型（VidFM，如WAN、Open-Sora、CogVideoX等）参数冻结，提取其时空特征。对于扩散模型，建议提取**中间网络层**和**早期但不含过多噪声的时间步**（mid-layer features with early-but-not-first timesteps）的激活值。
    2.  **探针训练**：使用提取的特征训练一个轻量级的Transformer探针模型（Probe），该模型包含交替注意力层和三个读出头（Read-out heads）。
*   **输出**：每帧的密集3D点云图（Point Maps）、深度图（Depth Maps）和相对相机位姿（Camera Poses）。
*   **硬件与资源**：需要GPU以运行视频大模型进行特征提取；作者表示将公开代码、数据和权重。

## 4. 主要创新点
1.  **基于直接属性预测的3D感知评估框架**：不同于以往依赖深度估计或多视图一致性等间接指标的研究，本文提出了一种直接的探测方法（Probing），通过简单的读出模块从冻结的视频特征中直接回归3D点云、绝对尺度一致的深度和相机位姿，从而更真实地反映模型内部的3D表征能力。
2.  **发现视频生成模型的“3D涌现”现象**：研究表明，仅在海量2D视频数据上训练的最先进视频生成模型（如WAN2.1-14B），其3D感知能力显著优于自监督图像模型（如DINOv2），并且在跨域数据（如DL3DV）上，其泛化能力甚至超过了专门针对3D任务训练的专家模型（如Fast3R）。
3.  **系统性的特征效用与最佳实践分析**：论文深入分析了时间推理（Temporal Reasoning）、3D微调、模型规模对3D感知的影响，并确定了扩散模型中提取3D特征的最佳层级和时间步配置。此外，还证明了在3D训练数据有限的情况下，使用视频模型特征进行前馈3D重建的效果远超基于DINO特征的方法。

## 5. 实验效果
在核心数据集 **CO3Dv2**（物体中心）和 **DL3DV**（复杂场景）上的评估结果如下：
*   **3D感知基准测试**：
    *   在CO3Dv2上，**WAN2.1-14B** 的表现仅次于在该领域内训练的3D专家模型 Fast3R（点云误差 0.284 vs 0.262），显著优于其他基线。
    *   在最具挑战性的 **DL3DV** 数据集（未在Fast3R训练集中）上，**WAN2.1-14B** 在所有指标上均**超越**了Fast3R（点云误差 1.051 vs 1.379），展现出极强的泛化能力。
*   **特征在重建任务中的应用**：
    *   通过替换VGGT模型中的DINO特征为WAN2.1特征（VidFM-VGGT），在仅使用不到 **10%** 的3D训练数据量时，其重建性能就已超过了使用100%数据训练的标准DINO-VGGT模型，证明了视频生成模型特征在低资源3D重建任务中的巨大潜力。


============================================================

## 📄 Spatia: Video Generation with Updatable Spatial Memory

- **链接**: https://huggingface.co/papers/2512.15716
- **阅读来源**: HTML

# Spatia: Video Generation with Updatable Spatial Memory

1. **应用领域**：
   计算机视觉 - 视频生成 (AIGC)、长时序视频合成、3D 场景理解与重建。

2. **一句话核心贡献**：
   提出了一种名为 Spatia 的视频生成框架，通过维护和迭代更新 3D 场景点云作为显式空间记忆，有效解决了长视频生成中因高维数据导致的长期空间一致性差和记忆丢失问题。

3. **使用指南**：
   *   **输入**：
       1.  **初始图像**：用于估计初始的 3D 场景点云。
       2.  **文本指令**：描述视频内容（支持动态实体描述）。
       3.  **相机轨迹**：指定相机的运动路径。
   *   **输出**：具有长时序空间一致性且包含动态实体的视频序列。
   *   **工作流程**：
       1.  从初始图像生成初始空间记忆（点云）。
       2.  **迭代生成**：用户指定新轨迹和文本 -> 系统根据当前点云渲染投影视频并检索历史参考帧 -> 模型生成新的视频片段。
       3.  **记忆更新**：利用 Visual SLAM（如 MapAnything）结合新生成的帧更新 3D 场景点云。
   *   **硬件需求**：论文中训练使用了 AMD MI250 GPU，推理过程涉及点云渲染和视频生成模型，预计需要较高显存的 GPU 支持。

4. **主要创新点**：
   *   **可更新的空间记忆机制 (Updatable Spatial Memory)**：不同于传统模型仅依赖隐空间上下文，Spatia 显式地维护一个 3D 场景点云作为记忆，并通过视觉 SLAM 算法在生成过程中不断更新这一记忆，从而实现对“已访问位置”的精准记忆。
   *   **动静解耦设计 (Dynamic–Static Disentanglement)**：将静态场景（作为空间记忆存储）与动态实体（由生成模型实时合成）分离。这使得模型既能保持背景的几何一致性，又能生成与场景交互的逼真动态物体。
   *   **基于几何的显式控制与编辑**：通过将 3D 点云投影为 2D 引导信号，实现了精确的相机轨迹控制；同时支持通过直接修改 3D 点云（如移除物体）来进行 3D 感知的交互式视频内容编辑。

5. **实验效果**：
   *   **数据集**：在 **WorldScore** 基准和 **RealEstate10K** 数据集上进行了评估。
   *   **核心表现**：
       *   **空间一致性**：在“闭环生成”（相机轨迹回到原点）测试中，Spatia 生成的最终帧与初始帧的匹配度（Match Accuracy）、PSNR 和 SSIM 均显著优于现有基线模型（如 Wan2.2），证明了其极强的抗几何漂移能力。
       *   **长视频质量**：在多片段自回归生成任务中，Spatia 成功保持了长时序的结构完整性，而对比方法则出现了明显的场景崩塌或形变。
       *   **视觉质量**：在保持高空间一致性的同时，未牺牲生成视频的视觉质量和动态效果。


============================================================
