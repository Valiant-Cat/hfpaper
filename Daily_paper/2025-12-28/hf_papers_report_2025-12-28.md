# Hugging Face Daily Papers Report
**Date**: 2025-12-28
**Source URL**: https://huggingface.co/papers/date/2025-12-28

============================================================

## 📄 Schoenfeld's Anatomy of Mathematical Reasoning by Language Models

- **链接**: https://huggingface.co/papers/2512.19995
- **阅读来源**: HTML

### 1. 应用领域
**NLP - 大语言模型推理 (LLM Reasoning) / 可解释性分析 (Interpretability) / 认知科学 (Cognitive Science)**

### 2. 一句话核心贡献
本文提出了一种基于认知科学（Schoenfeld片段理论）的分析框架 **ThinkARM**，将大语言模型的思维链（CoT）抽象为细粒度的功能性推理步骤（如探索、验证、计划等），从而不仅通过准确率，更通过内在思维结构和动态模式来系统地剖析、比较和诊断模型的推理能力。

### 3. 使用指南
*   **输入**：大语言模型针对数学问题生成的完整推理轨迹（Reasoning Traces / Chain-of-Thought）。
*   **处理流程**：
    1.  **分句**：将推理文本分割为独立的句子。
    2.  **自动标注**：利用高性能LLM（论文中推荐GPT-5，因其与人类标注一致性最高）作为自动标注器。
    3.  **提示词工程**：输入论文提供的详细**标注指南（Guidebook）**，包含8个认知类别（读题、分析、计划、执行、探索、验证、监控、回答）的定义及示例。
*   **输出**：每一句话对应的认知片段标签，以及基于这些标签生成的推理阶段分布图、状态转移矩阵和N-gram模式分析。
*   **资源**：论文构建了包含18,243个问题及其推理轨迹的语料库，并建立了一个经人工验证的“金标准”数据集用于评估标注器性能。

### 4. 主要创新点
1.  **理论迁移与扩展**：创造性地采用数学教育家 Alan Schoenfeld 的**片段理论（Episode Theory）**作为中间层抽象工具，将人类解决问题的认知模型迁移至大模型分析，并扩展了“Monitor（监控）”和“Answer（回答）”两个类别以适应LLM特性。
2.  **ThinkARM 自动化分析框架**：构建了一套可扩展的自动化标注管道，通过对比多种模型（如GPT-4o, Gemini-Pro），验证了使用顶尖LLM（如GPT-5）进行句子级细粒度认知标注的可行性和高一致性，解决了大规模分析的成本问题。
3.  **深层推理模式的量化诊断**：通过该框架揭示了“推理模型”与“非推理模型”的结构性差异（如推理模型存在显著的“探索-监控”循环），并量化了模型“过度思考”与“高效推理”在认知层面的具体表现（例如高效模型倾向于选择性抑制验证环节）。

### 5. 实验效果
在基于 **Omni-MATH** 数据集构建的语料库上，通过对 DeepSeek-R1、Phi-4、Qwen3-32B 等多个模型的分析，得出以下核心发现：
*   **标注一致性**：在包含4,670个句子的人工标注金标准集上，GPT-5 展现了与人类标注者最高的一致性（Kappa系数最高），证明了自动标注的可靠性。
*   **推理结构差异**：**推理模型**（如DeepSeek-R1）在思维链中表现出“三段式心跳”模式（分析/探索 -> 执行 -> 验证/回答），且包含频繁的“探索-监控”迭代循环；而**非推理模型**则主要由线性的“执行”步骤主导。
*   **正确性关联**：案例研究表明，正确解通常伴随着将“探索（Explore）”成功转化为“监控（Monitor）”或“再分析”的行为，而错误解往往在探索后直接终止或强行执行。
*   **效率模型分析**：对于经过效率优化的模型（如L1, ThinkPrune），分析发现它们并非均匀地缩短回答，而是选择性地大幅削减了“验证”和“反馈循环”相关的片段，从而导致认知结构的改变。


============================================================

## 📄 Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning

- **链接**: https://huggingface.co/papers/2512.20605
- **阅读来源**: HTML

# 研究论文报告：自回归模型中的涌现时间抽象赋能分层强化学习

### 1. 应用领域
**强化学习 (Reinforcement Learning)**，具体涉及**分层强化学习 (Hierarchical RL)** 以及**自回归序列模型**（如 Transformer 和 State-Space Models）在复杂长程任务中的控制与规划。

### 2. 一句话核心贡献
本文提出了一种名为“内部强化学习”（Internal RL）的新范式，通过在冻结的预训练自回归模型内部（残差流）发现并控制时间抽象的潜在动作，成功解决了传统 Token 级强化学习在稀疏奖励和长程任务中探索效率极低的问题。

### 3. 使用指南
*   **输入数据**：
    1.  一个在大规模行为数据集（包含观察-动作序列，但无显式奖励或目标标签）上预训练好的自回归模型（基座模型）。
    2.  一个新的下游任务环境，该环境通常具有稀疏奖励和层级结构。
*   **操作流程**：
    1.  **预训练阶段**：训练基座模型进行下一个 Token（动作）预测，使其内部隐式学习到任务的子目标结构。
    2.  **元控制器训练**：冻结基座模型，训练一个无监督的“元控制器”（Metacontroller）。该控制器读取基座模型的残差流激活值，并输出线性控制向量干预残差流。它通过变分推断学习生成稀疏切换的潜在代码（Latent Codes）。
    3.  **内部强化学习**：将基座模型和部分元控制器视为环境的一部分，使用强化学习算法（如改进版的 PPO/GRPO）直接优化元控制器的潜在代码输入。由于潜在代码是在抽象时间尺度上操作的，探索空间被大幅压缩。
*   **输出结果**：一个能够执行长程规划、自动分解子目标并完成稀疏奖励任务的智能体策略。

### 4. 主要创新点
1.  **内部强化学习 (Internal RL) 机制**：
    与传统的在原始动作空间（Token-level）进行微调不同，本文提出直接在预训练模型的**残差流（Residual Stream）**中进行强化学习。通过干预内部激活值，利用模型预训练中涌现的高层语义特征，将高维、高频的控制问题转化为低维、低频（时间抽象）的潜在空间决策问题。
2.  **无监督时序抽象发现**：
    设计了一种特殊的**元控制器架构**，包含一个递归的“切换单元”（Switching Unit）和一个在训练时利用未来信息的非因果编码器。该架构能够在没有任何人工标签的情况下，自动从连续行为中发现具有语义意义的“子目标”（如导航中的特定颜色地标），并学会何时切换这些子目标。
3.  **冻结基座模型的必要性证明**：
    通过率失真（Rate-Distortion）分析，论文揭示了一个反直觉的现象：相比于联合训练（Co-training），**控制一个冻结的预训练模型**对于发现有效的时间抽象至关重要。预训练过程在模型中间层构建了线性可解码的信念状态，冻结参数有助于保留这些结构供元控制器利用。

### 5. 实验效果
*   **数据集与环境**：
    实验在两个具有组合性层级结构的任务中进行：离散的 **Grid World** 和基于物理模拟的连续控制 **MuJoCo Ant** 环境。这些任务要求智能体按特定顺序访问一系列子目标，且仅在完成整个序列后获得一次稀疏奖励。
*   **核心表现**：
    *   **基线对比**：在这些任务中，标准的强化学习微调（包括 GRPO 算法）以及之前的分层 RL 方法（如 CompILE）完全失败，成功率接近 **0%**，主要原因是无法在稀疏奖励下进行有效探索。
    *   **Internal RL 表现**：本文提出的 Internal RL 方法在所有测试任务中均取得了极高的成功率（在 Ant 环境中接近 **100%** 的任务完成率）。
    *   **泛化能力**：实验表明，学习到的内部控制器具有组合泛化能力，能够解决预训练期间未见过的子目标组合顺序和更长的任务序列。


============================================================

## 📄 VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation

- **链接**: https://huggingface.co/papers/2512.19680
- **阅读来源**: HTML

# VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation 研究报告

1. **应用领域**
   计算机视觉-图像生成（Computer Vision - Image Generation）、自回归模型微调、多模态生成（如文生图）、强化学习应用（RL for Generation）。

2. **一句话核心贡献**
   提出了一种名为 VA-π 的轻量级后训练框架，通过变分策略优化将自回归生成器的 Token 分布直接与像素空间分布对齐，解决了传统 AR 模型因缺乏像素级监督而导致的生成质量下降问题。

3. **使用指南**
   *   **输入**：预训练的自回归视觉生成模型（如 LlamaGen、Janus-Pro）以及少量对应的训练数据（如图片或图文对）。
   *   **输出**：经过微调优化的 AR 生成器模型权重，能生成保真度和一致性更高的图像。
   *   **流程**：该方法作为一个后训练（Post-training）步骤。用户无需重训 Tokenizer 或训练外部奖励模型。通过结合教师强制（Teacher Forcing）采样和强化学习（GRPO），利用图像重建误差作为奖励信号更新模型。
   *   **硬件与效率**：训练极其高效，在 8 张 A100 GPU 上仅需约 25 分钟，使用预训练数据集的 1% 即可完成微调。
   *   **代码**：论文已提及代码开源（Code is available）。

4. **主要创新点**
   1.  **统一的变分对齐目标（ELBO）**：将生成器与分词器的对齐问题公式化为变分优化，推导出一个包含“像素重建项”和“先验正则化项”的证据下界（ELBO），从理论上连接了离散 Token 建模与连续像素生成。
   2.  **基于内在奖励的 RL 优化**：不同于通过 STE（直通估计器）仅优化 Ground-truth 路径，该方法将 AR 生成器视为策略，将像素空间的重建质量（MSE + 感知损失）作为内在奖励，利用 GRPO 算法进行策略梯度更新，从而直接优化采样轨迹。
   3.  **噪声上下文正则化**：为了防止模型偏离预训练分布（KL 散度约束），引入了基于含噪上下文的 Next-Token Prediction 作为正则化项，在不增加额外推理成本的情况下有效保持了模型的生成能力并防止模式崩塌。

5. **实验效果**
   该方法在类条件图像生成（C2I）和文本生成图像（T2I）任务上均取得了显著提升，验证数据集包括 ImageNet-1K 和 GenEval：
   *   **图像质量大幅提升**：在 LlamaGen-XXL (1.4B) 模型上，FID 从 **14.36 降至 7.65**，IS（Inception Score）从 **86.55 提升至 116.70**（无 CFG 设置）。
   *   **多模态泛化性**：在统一多模态模型 Janus-Pro 1B 上，GenEval 评分从 0.725 提升至 0.744，在计数、属性绑定等复杂任务上表现更佳。
   *   **极致效率**：相比传统的 AR-GRPO 方法，仅需 **13.4%** 的计算成本，且无需任何外部奖励模型（如人工偏好模型）。


============================================================

## 📄 GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

- **链接**: https://huggingface.co/papers/2512.13043
- **阅读来源**: HTML

# GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training

### 1. 应用领域
**多模态大模型 (Multimodal LLM / VLM)**、**强化学习 (Reinforcement Learning)**、**具身智能体 (Embodied Agents)**。

### 2. 一句话核心贡献
提出了一种名为 GTR-Turbo 的高效训练框架，通过融合 RL 训练过程中产生的历史 Checkpoint 权重构建“免费”且强大的教师模型，替代了昂贵的外部商用模型（如 GPT-4）进行思维链指导，在大幅降低训练成本和时间的同时实现了 SOTA 性能。

### 3. 使用指南
*   **输入数据**：视觉观测（图像）与任务文本指令。
*   **核心流程**：
    1.  **初始化**：使用经过 SFT 初始化具备基础指令遵循能力的 VLM（如 Qwen2.5-VL）。
    2.  **模型融合**：在 RL 训练过程中建立缓冲区保存历史 Checkpoint，利用 **TIES Merging** 技术动态融合这些权重生成 Teacher 模型。
    3.  **训练更新**：Teacher 模型对 Student 模型（当前训练步）生成的“思维（Thought）”部分进行指导。
    4.  **指导方式**：通过最小化 SFT 损失或计算反向 KL 散度（作为辅助奖励）来优化模型，同时结合 PPO 算法更新动作（Action）策略。
*   **硬件需求**：论文实验中使用 2 张 NVIDIA GPU，一张用于部署融合后的 Teacher 模型进行推理，另一张用于训练 Student 模型。
*   **适用场景**：适用于存在稀疏奖励、长视界推理任务的视觉智能体训练（如卡牌游戏、具身导航）。

### 4. 主要创新点
1.  **基于模型融合的自演进教师机制**：
    打破了以往依赖昂贵闭源模型（如 GPT-4、Gemini）作为教师的范式，通过聚合 RL 训练轨迹中的历史权重，零成本地构建出性能优于当前策略的教师模型，解决了“思维坍塌（Thought Collapse）”问题。
2.  **基于反向 KL 散度的软 Logit 蒸馏**：
    提出使用反向 KL 散度（Reverse KL Divergence）代替传统的硬 SFT 标签进行指导。该方法利用了 Token 级别的概率分布信息，仅需一次前向传播即可计算，不仅计算效率高，还通过“寻模（Mode-seeking）”特性鼓励模型在保持探索性的同时对齐教师分布。
3.  **引入 TIES Merging 消除参数干扰**：
    针对直接平均权重可能导致的参数干扰问题，引入 TIES（Trimming, Electing Signs, Selective Averaging）融合技术，通过去除冗余参数和解决符号冲突，确保了融合模型的稳定性和性能优势。

### 5. 实验效果
在两个具有挑战性的视觉智能体基准数据集上进行了验证：
*   **Points24（24点游戏，需视觉识别与数学推理）**：
    *   **性能**：GTR-Turbo (KL版) 取得了 SOTA 性能，超越了依赖 GPT-4o 作为修正器的原始 GTR 方法以及 RL4VLM 基线。
    *   **效率**：相比 GTR，训练时间减少了 50%，计算成本降低了 60%。
*   **ALFWorld（具身交互环境，长序列决策）**：
    *   **性能**：在完全不依赖外部 API 的情况下，达到了与 GTR 相当的任务成功率，且远超同等规模的其他基线模型。
    *   **兼容性**：在使用最新的 Qwen3-VL 模型时，展示了进一步的性能提升，甚至在零样本 SFT 初始化的情况下也能有效运行。


============================================================

## 📄 How Much 3D Do Video Foundation Models Encode?

- **链接**: https://huggingface.co/papers/2512.19949
- **阅读来源**: HTML

# 论文研报：How Much 3D Do Video Foundation Models Encode?

## 1. 应用领域
**计算机视觉 - 视频基础模型表征分析与3D重建** (Computer Vision - Video Foundation Models Analysis & 3D Reconstruction)

## 2. 一句话核心贡献
提出了首个模型无关的评估框架，通过浅层探测器量化了预训练视频基础模型（VidFMs）的3D感知能力，揭示了SOTA视频生成模型（如WAN2.1）即使仅在2D数据上训练，也涌现出了超越专业3D模型的强大且可泛化的3D结构理解能力。

## 3. 使用指南
*   **输入数据**：一段标准的RGB视频剪辑。
*   **核心步骤**：
    1.  **特征提取**：将视频输入预训练好的视频基础模型（如WAN2.1-14B, Open-Sora2.0等），保持模型参数冻结。对于扩散模型，需执行单步去噪并在特定时间步（通常是早期但不为0的时间步）和特定网络层（通常是中间层）提取隐藏层激活特征。
    2.  **探测训练**：使用提取的时空特征训练一个轻量级的浅层Transformer探测器（Probe），该探测器包含交替注意力机制和三个读出头（Read-out heads）。
*   **输出结果**：逐帧的稠密3D点云图（Point Maps）、深度图（Depth Maps）以及相对相机位姿（Camera Poses）。
*   **开源情况**：作者明确表示将公开代码、数据和权重。
*   **硬件需求**：由于涉及运行14B参数量的视频生成模型提取特征，需要高性能GPU支持。

## 4. 主要创新点
1.  **首个针对VidFMs的3D感知量化基准**：不同于以往使用深度图或多视图一致性作为间接代理的评估方法，本文构建了一个包含点云、深度和相机位姿的直接3D属性探测框架，并在CO3Dv2和DL3DV数据集上对主流视频模型进行了系统性基准测试。
2.  **揭示了视频生成模型涌现的3D能力**：研究发现，尽管仅在2D视频上训练，顶尖的视频生成模型（如WAN2.1-14B）展现出极强的全局3D理解能力，其表现优于大规模图像模型（如DINOv2），甚至在跨域测试中超越了专门训练的3D专家模型（如Fast3R）。
3.  **确定了扩散模型最佳3D特征提取策略**：系统性地分析了扩散模型内部特征的分布，发现**中间层（Mid-layers）**配合**早期去噪时间步（Early timesteps）**能提取到最丰富的3D几何信息；并证明了利用这些VidFM特征进行小样本3D重建比传统的DINO特征更有效。

## 5. 实验效果
在**CO3Dv2**（物体级）和**DL3DV**（场景级）数据集上的核心实验结果如下：
*   **3D感知能力对比**：在DL3DV数据集上，**WAN2.1-14B** 的点云重建误差（Point Error）为 **1.051**，优于专门设计的3D模型 **Fast3R**（误差 1.379）和自监督视频模型 **V-JEPA**（误差 1.576）。
*   **图像 vs. 视频模型**：图像基础模型 **DINOv2** 在全局3D结构恢复上表现糟糕（DL3DV误差 2.814），证明了视频模型中的**时间推理（Temporal Reasoning）**对于形成全局3D理解至关重要。
*   **模型规模影响**：模型扩展对3D感知有显著正向影响，WAN从1.3B扩展到14B参数后，点云误差大幅下降（从1.670降至1.051）。
*   **低资源重建优势**：在使用VidFM特征（WAN2.1-14B）替换VGGT模型中的DINO特征后，仅使用不到10%的3D训练数据，其性能即可超越使用100%数据训练的原始DINO-VGGT基线。


============================================================

## 📄 Latent Implicit Visual Reasoning

- **链接**: https://huggingface.co/papers/2512.21218
- **阅读来源**: HTML

# Latent Implicit Visual Reasoning (LIVR) 论文报告

### 1. 应用领域
**多模态大模型 (Large Multimodal Models, LMMs)**、**视觉推理 (Visual Reasoning)**、**视觉问答 (Visual Question Answering, VQA)**。

### 2. 一句话核心贡献
提出了一种名为 LIVR 的任务无关机制，通过引入隐式潜在 token (Latent Tokens) 和独特的“视觉瓶颈”训练策略，使多模态模型在**无需显式中间监督**（如边界框、辅助图像）的情况下，自主学习并利用高维视觉抽象，显著提升了处理复杂视觉任务的能力。

### 3. 使用指南
*   **输入**：一张或多张参考图像 + 文本提示（Prompt，通常为多项选择题形式）。
*   **输出**：文本答案（如选项字母或计数结果）。
*   **模型修改**：在原始文本提示后附加 $K$ 个可学习的潜在 token（Latent Tokens），这些 token 在嵌入层初始化但在训练中解冻。
*   **训练流程**：无需额外标注数据，仅使用问答对数据进行两个阶段微调：
    1.  **阶段一（视觉瓶颈训练）**：修改注意力掩码，使得“答案 token”只能关注“潜在 token”，不能直接关注“图像 token”；同时“提示 token”也不能关注“图像 token”。这强制潜在 token 充当视觉信息的唯一载体。
    2.  **阶段二（联合训练）**：恢复标准注意力掩码，允许答案同时关注原始图像和潜在 token，进行最终微调。
*   **硬件需求**：实验中使用 NVIDIA RTX 6000 Ada GPU，适用于标准大模型微调的硬件配置。

### 4. 主要创新点
1.  **无监督隐式视觉推理 (Implicit Visual Reasoning without Supervision)**：与先前依赖显式监督（如提供中间步骤的文字链、边界框、深度图或辅助图像）的方法不同，LIVR 不需要人工定义的中间视觉目标，直接通过任务目标的端到端训练来学习有用的视觉抽象，降低了数据标注成本并避免了人工偏见。
2.  **视觉瓶颈机制 (Visual Bottlenecking)**：设计了一种新颖的注意力掩码策略（Attention Masking），在训练初期切断文本与原始图像的直接联系，强制模型将关键视觉信息压缩并编码到新增的潜在 token 中，从而激发模型在潜在空间（Latent Space）进行非语言的视觉计算。
3.  **突破语言瓶颈 (Overcoming Language Bias)**：解决了现有 LMM 强迫将所有视觉信息转化为文本进行推理的局限性。通过引入非文本的潜在 token，模型能够保留难以用语言描述的空间结构和抽象视觉特征（如拼图、几何对应关系），从而处理“以视觉为中心”的复杂任务。

### 5. 实验效果
在 9 个以视觉感知为核心的任务（基于 BLINK 基准改编，包括计数、拼图、对象定位、语义/功能/视觉对应、艺术风格等）上进行了评估：
*   **单任务微调**：在 Qwen2.5-VL-3B 模型上，LIVR 相比直接监督微调（Direct SFT）平均准确率提升了 **6.24%**。在需要复杂视觉抽象的任务上提升尤为明显，如拼图任务（Jigsaw）提升 **12%**，功能对应任务（Functional Correspondence）提升 **13.02%**。
*   **对比 SOTA**：与依赖显式辅助图像监督的方法 Mirage 相比，LIVR 在无显式监督的情况下取得了压倒性优势。例如在 Jigsaw 任务上比 Mirage 高出 **19.4%**，在视觉空间规划（VSP）任务上高出 **20.0%**。
*   **多任务泛化**：在 Qwen3-VL-4B 的多任务混合训练设置下，LIVR 依然全面优于直接微调，证明了该方法的通用性和鲁棒性。
*   **机制验证**：消融实验表明，模型确实依赖潜在 token 进行推理；如果在推理时移除这些 token，准确率会显著下降。可视化注意力图显示，潜在 token 能够自动聚焦于解题所需的关键视觉区域（如对应的物体部件或计数对象）。


============================================================

## 📄 Spatia: Video Generation with Updatable Spatial Memory

- **链接**: https://huggingface.co/papers/2512.15716
- **阅读来源**: HTML

# 论文阅读报告：Spatia: Video Generation with Updatable Spatial Memory

1. **应用领域**：
   计算机视觉 - 视频生成（特别是长时视频生成、3D场景一致性视频合成、图像转视频）。

2. **一句话核心贡献**：
   提出了一种名为 Spatia 的框架，通过维护和迭代更新显式的 3D 场景点云作为持久空间记忆，解决了现有视频生成模型在长时生成过程中因高维信号复杂性导致的空间一致性丢失和几何结构漂移问题。

3. **使用指南**：
   *   **输入**：
       1.  **初始条件**：一张初始图像（用于估计初始 3D 场景点云）。
       2.  **控制信号**：文本指令（描述内容）和 3D 相机轨迹（定义视角移动）。
       3.  **迭代输入**：在后续生成中，输入前一阶段生成的视频片段。
   *   **流程**：
       1.  从初始图像或当前场景生成 3D 点云记忆。
       2.  根据用户指定的相机轨迹，将点云投影为 2D 序列作为几何条件。
       3.  检索与当前视角相关的参考帧。
       4.  模型结合文本、参考帧、前序片段和投影视频生成新的视频片段。
       5.  利用 Visual SLAM（如 MapAnything）使用新生成的帧更新 3D 点云记忆，用于下一轮生成。
   *   **输出**：具有长时空间一致性、几何结构稳定且包含动态实体的长视频。
   *   **硬件与实现**：核心模型参数量约为 5B，训练涉及 ControlNet 和 LoRA 微调，使用了 64 张 AMD MI250 GPU，推理过程涉及点云处理和 SLAM 算法，预计需要高性能 GPU。

4. **主要创新点**：
   1.  **可更新的显式空间记忆（Updatable Spatial Memory）**：不同于依赖隐式注意力的传统方法，Spatia 维护一个显式的 3D 场景点云，并通过 Visual SLAM 算法利用新生成的帧不断更新该点云，从而使模型能“记住”并一致地重绘之前的场景区域。
   2.  **动静分离生成策略（Dynamic–Static Disentanglement）**：将静态场景结构作为空间记忆存储，而将动态实体（人、物体的移动）主要交由生成模型处理。这种解耦设计既保证了背景的几何稳定性，又不影响生成逼真的动态交互内容。
   3.  **3D 感知的交互式编辑与控制**：由于生成过程以 3D 点云为条件，支持显式的相机轨迹控制（类似 3DGS 渲染路径），并允许用户通过直接编辑 3D 点云（如移除物体、修改颜色）来精确控制生成的视频内容。

5. **实验效果**：
   *   **数据集**：在 **WorldScore** 和 **RealEstate10K** 数据集上进行了评估。
   *   **性能表现**：
       *   **长时一致性**：在闭环视频生成测试（相机回到原点）中，Spatia 展现了优越的空间保持能力，相比基线模型（如 Wan2.2）显著减少了几何漂移，终帧与首帧的匹配精度（Match Accuracy）和视觉相似度（PSNR, SSIM, LPIPS）更高。
       *   **视觉质量**：在生成包含动态实体的视频时，未牺牲视觉质量，WorldScore 指标显示其在静态场景保持和动态质量上均表现出色。
       *   **消融实验**：证明了引入场景点云投影和参考帧检索机制对于维持长时空间一致性至关重要。


============================================================
